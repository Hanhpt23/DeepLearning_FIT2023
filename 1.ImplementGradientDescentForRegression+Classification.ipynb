{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AL7LdLq0w7-m",
        "St91Qp_tZWoX",
        "Ftu3Y1I0ZUOR",
        "THTdupPbc-OD",
        "-Uj2BaC7jK-Z",
        "lYlzGjc7bcb6",
        "h8CV0fKo0Cww",
        "mIJwWPC9a_Kh",
        "DkKNLtUm1Ogu",
        "9T13g_c_1XgV",
        "aERVwGhWsQys",
        "PuJbqQ94SYGT",
        "kagdpsSz6qYL",
        "xshXeCFxck9T",
        "8CPccA4B562E",
        "bVKGFHO-vwDn",
        "0-yaUco1vmqF",
        "LUzTBeLK6CgM"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO5s+bPAFS1jnXUOklk6FtX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hanhpt23/DeepLearning_FIT2023/blob/main/1.ImplementGradientDescentForRegression%2BClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Contents:\n",
        "1. Implement numerical Gradient Descent (GD) from scratch\n",
        "2. Implement explicit GD from the [Least Squares](https://en.wikipedia.org/wiki/Least_squares) loss function\n",
        "3. Implement 3 variants of the each GD\n",
        "4. Building the model using the GD for linear regression following the [Sklearn](https://scikit-learn.org/stable/)'s style\n",
        "5. Building a model using the GD for binary classification\n",
        "6. Buidling a model using the GD for mutil-class classification\n",
        "7. Test the model on the [MNIST](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html) dataset"
      ],
      "metadata": {
        "id": "HYwZWNUtbjuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import necessary packages and libraries (numpy and sklearn)"
      ],
      "metadata": {
        "id": "AL7LdLq0w7-m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2bWljFOAgXI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.preprocessing import scale\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# increase the width of boxes in the notebook file (this is only cosmetic)\n",
        "np.set_printoptions(linewidth=180)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 1 Gradient Descent"
      ],
      "metadata": {
        "id": "0ndJze8BeEBW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Gradient descent with an option to use the explicit gradient formula of a loss function"
      ],
      "metadata": {
        "id": "St91Qp_tZWoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing numerical gradient descent and explicit gradient descent"
      ],
      "metadata": {
        "id": "Xg0krHb4ZmIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# estimate the gradient\n",
        "def computeGradient(f, x, h):\n",
        "    n = len(x)\n",
        "    gradient = np.zeros(n)\n",
        "\n",
        "    for counter in range(n):\n",
        "        xUp = x.copy()\n",
        "        xUp[counter] += h\n",
        "        gradient[counter] = (f(xUp) - f(x))/h\n",
        "\n",
        "    return gradient\n",
        "\n",
        "# Explicit gradient\n",
        "def compute_explicit_gradient(X, y, w):\n",
        "    # Calculate the gradient of the function with respect to w\n",
        "    gradient = 2 * np.dot(X.T, (np.dot(X, w) - y))\n",
        "    theta = np.linalg.inv(X.T@X)@X.T@y\n",
        "    return gradient"
      ],
      "metadata": {
        "id": "98xJzin5AjhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test output from the two gradient methods"
      ],
      "metadata": {
        "id": "u2LXKXKxXSrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[6], [7], [8], [9], [7] ])\n",
        "y = np.array([1, 2, 3, 3, 4])\n",
        "x0 = [0, 0]\n",
        "X = np.hstack((np.ones([X.shape[0], 1]), X))\n",
        "L = lambda w: ((X @ w).T - y.T) @ (X @ w - y)\n",
        "\n",
        "# test numerical gradient\n",
        "gradient_num = computeGradient(L, x0, 0.001)\n",
        "\n",
        "# test explicit gradient\n",
        "gradient_exp = compute_explicit_gradient(X, y, x0)\n",
        "print('Numerical gradient: ', gradient_num, '\\n',\n",
        "      'Explicit gradient: ', gradient_exp)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI7tI2lpXOh0",
        "outputId": "2d5cec9a-3b42-4987-8882-f591a1fb59b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numerical gradient:  [ -25.995 -197.721] \n",
            " Explicit gradient:  [ -26. -198.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizing parameters from gradients using two method by declare that\n",
        "\n",
        "use_explicit_gradient = True or False"
      ],
      "metadata": {
        "id": "x8Nq0QQYXdl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run gradient descent and output the coordinates of the estimated critical point\n",
        "def gradientDescent(f, y, x0, alpha, h, tolerance, maxIterations, use_explicit_gradient = False):\n",
        "    # set x equal to the initial guess\n",
        "    x = x0\n",
        "\n",
        "    # take up to maxIterations number of steps\n",
        "    for counter in range(maxIterations):\n",
        "        # choosing gradient method\n",
        "        if use_explicit_gradient:\n",
        "          gradient = compute_explicit_gradient(X, y, x)\n",
        "        else:\n",
        "          gradient = computeGradient(f, x, h)\n",
        "\n",
        "        # stop if the norm of the gradient is near 0 (success)\n",
        "        if np.linalg.norm(gradient) < tolerance:\n",
        "            print('Gradient descent took', counter, 'iterations to converge')\n",
        "            # print('The norm of the gradient is', np.linalg.norm(gradient))\n",
        "\n",
        "            # return the approximate critical point x\n",
        "            return x\n",
        "\n",
        "        # print a message if we do not converge (failure)\n",
        "        elif counter == maxIterations-1:\n",
        "            print(\"Gradient descent failed\")\n",
        "            # print('The gradient is', gradient)\n",
        "\n",
        "            # return x, sometimes it is still pretty good\n",
        "            return x\n",
        "\n",
        "        # take a step in the opposite direction as the gradient\n",
        "        x -= alpha*gradient"
      ],
      "metadata": {
        "id": "JVq9me8eBRzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test converenge of the model using the numerical gradient\n"
      ],
      "metadata": {
        "id": "E4VIdS5RZz9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[6], [7], [8], [9], [7] ])\n",
        "y = np.array([1, 2, 3, 3, 4])\n",
        "x0 = [0, 0]\n",
        "X = np.hstack((np.ones([X.shape[0], 1]), X))\n",
        "L = lambda w: ((X @ w).T - y.T) @ (X @ w - y)\n",
        "\n",
        "# numerical gradient\n",
        "x_params = gradientDescent(L, y, x0, alpha = 0.001, h = 0.001, tolerance = 0.01, maxIterations = 10000000, use_explicit_gradient = False)\n",
        "print('x_params', x_params)\n",
        "print('Prediction y_hat using numerical gradient: \\n', X@x_params)\n",
        "print(f'Loss of the prediction value is {((y-X@x_params)**2).sum(axis =0)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXdMKA00A6ax",
        "outputId": "d65f6402-eafc-4b36-a918-94c961e857b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient descent took 17180 iterations to converge\n",
            "x_params [-1.15880201  0.5080126 ]\n",
            "Prediction y_hat using numerical gradient: \n",
            " [1.88927362 2.39728623 2.90529883 3.41331144 2.39728623]\n",
            "Loss of the prediction value is 3.697130014379029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test converenge of the model using the explicit gradient"
      ],
      "metadata": {
        "id": "NaIq5txh4vb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# explicit gradient\n",
        "x_params = gradientDescent(L, y, x0, alpha = 0.001, h = 0.001, tolerance = 0.01, maxIterations = 100000, use_explicit_gradient = True)\n",
        "print('x_params', x_params)\n",
        "print('Prediction y_hat using explicit gradient: \\n', X@x_params)\n",
        "print(f'Loss of the prediction value is {((y-X@x_params)**2).sum(axis =0)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1OiZvKqS_Za",
        "outputId": "99e4184e-9fd7-4853-d6a0-d3e52ed26611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient descent took 17878 iterations to converge\n",
            "x_params [-1.33049327  0.5312817 ]\n",
            "Prediction y_hat using explicit gradient: \n",
            " [1.85719691 2.38847861 2.91976031 3.451042   2.38847861]\n",
            "Loss of the prediction value is 3.6925806662475953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Choosing n random starting points and output the parameters resulting in minimum training loss across all the runs."
      ],
      "metadata": {
        "id": "Ftu3Y1I0ZUOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_optimization(num_points, f, y, alpha, h, tolerance, maxIterations, explicit = True):\n",
        "    best_params = None\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for _ in range(num_points):\n",
        "        # Generate a random starting point\n",
        "        initial_theta = np.random.rand(2)  # Assuming 2 parameters (x and y)\n",
        "        print('Initial starting points: ', initial_theta )\n",
        "\n",
        "        # Perform gradient descent\n",
        "        optimized_theta = gradientDescent(f, y, initial_theta, alpha, h, tolerance, maxIterations,  use_explicit_gradient = explicit)\n",
        "\n",
        "        # Calculate the loss for the optimized parameters\n",
        "        current_loss = f(optimized_theta)\n",
        "\n",
        "        # Check if this run has the lowest loss so far\n",
        "        if current_loss < best_loss:\n",
        "            best_loss = current_loss\n",
        "            best_params = optimized_theta\n",
        "\n",
        "    return best_params, best_loss"
      ],
      "metadata": {
        "id": "YT42tds-ZRrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test model"
      ],
      "metadata": {
        "id": "1Yn0Apnc43j2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_params, best_loss = run_optimization(num_points = 2, f = L, y = y, alpha = 0.001, h = 0.001, tolerance = 0.01, maxIterations = 10000000)\n",
        "print(f'best_params {best_params} with the loss of {best_loss}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yc9TSvrVa8m4",
        "outputId": "6bbc2866-3949-4a05-fc20-cc827cf4b8a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial starting points:  [0.00691004 0.74718517]\n",
            "Gradient descent took 17521 iterations to converge\n",
            "Initial starting points:  [0.45673406 0.17185419]\n",
            "Gradient descent took 19302 iterations to converge\n",
            "best_params [-1.33049495  0.53128192] with the loss of 3.6925806493106315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Write a version with a cyclic learning rate that changes in each training epoch and saves the model parameters every time the learning rate vanishes. Then, select the best parameters observed."
      ],
      "metadata": {
        "id": "THTdupPbc-OD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent_lrSchedual(num_epochs, num_points, f, y, x0, alpha, h, tolerance, maxIterations):\n",
        "    # Gradient Descent optimization algorithm with cyclic learning rate.\n",
        "    x = x0\n",
        "    best_params = None\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        learning_rate = alpha / (epoch + 1) # reduce learning rate for every epoch\n",
        "        print('Epoch ', epoch)\n",
        "\n",
        "        # x = gradientDescent(f, y, x0, learning_rate, h, tolerance, maxIterations,  use_explicit_gradient = True)\n",
        "        x, loss = run_optimization(num_points, f, y, alpha, h, tolerance, maxIterations, explicit = True)\n",
        "\n",
        "        # Save parameters every time the learning rate vanishes\n",
        "        np.savetxt(f\"model_parameters_epoch_{epoch}.txt\", x)\n",
        "\n",
        "        # Calculate the loss for the optimized parameters\n",
        "        current_loss = loss\n",
        "\n",
        "        # Check if this run has the lowest loss so far\n",
        "        if current_loss < best_loss:\n",
        "            best_loss = current_loss\n",
        "            best_params = x\n",
        "            best_epoch = epoch\n",
        "\n",
        "    print('Finding the best parameters at the epoch of ', best_epoch)\n",
        "\n",
        "    return best_params, best_loss\n"
      ],
      "metadata": {
        "id": "2vLqbFmUc_EL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test model"
      ],
      "metadata": {
        "id": "-DGz2aow4-wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 3\n",
        "num_points = 2 # sets initial values\n",
        "best_params, best_loss = gradient_descent_lrSchedual(num_epochs, num_points, f = L, y= y, x0 = x0, alpha = 0.001, h = 0.001, tolerance = 0.01, maxIterations = 10000000)\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Loss:\", best_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cL-Vz7a3dMCU",
        "outputId": "bededb5a-4993-4070-acaf-6c54e9f0a689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  0\n",
            "Initial starting points:  [0.1962297  0.19137485]\n",
            "Gradient descent took 18484 iterations to converge\n",
            "Initial starting points:  [0.90674916 0.62421733]\n",
            "Gradient descent took 20327 iterations to converge\n",
            "Epoch  1\n",
            "Initial starting points:  [0.73180306 0.38438901]\n",
            "Gradient descent took 19973 iterations to converge\n",
            "Initial starting points:  [0.15861568 0.70958004]\n",
            "Gradient descent took 18115 iterations to converge\n",
            "Epoch  2\n",
            "Initial starting points:  [0.90060792 0.00588006]\n",
            "Gradient descent took 20505 iterations to converge\n",
            "Initial starting points:  [0.87622799 0.391725  ]\n",
            "Gradient descent took 20327 iterations to converge\n",
            "Finding the best parameters at the epoch of  0\n",
            "Best Parameters: [-1.33050136  0.53128277]\n",
            "Best Loss: 3.6925805845682995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Write function for prediction for all models and averages the output predictions"
      ],
      "metadata": {
        "id": "-Uj2BaC7jK-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Combining all functions into a class (Model)\n"
      ],
      "metadata": {
        "id": "lYlzGjc7bcb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model():\n",
        "    # fit the model to the data\n",
        "    def fit(self, num_epochs, num_points, X, y, w0, alpha, h, tolerance, max_iterations, gradient_type: str, explicit = False):\n",
        "\n",
        "        X = np.hstack((np.ones([X.shape[0], 1]), X))\n",
        "\n",
        "        # find the w values that minimize the sum of squared errors via gradient descent\n",
        "        L = lambda w: ((X @ w).T - y.T) @ (X @ w - y)\n",
        "\n",
        "        if gradient_type == 'simple':\n",
        "          print(f'------------\\nUsing the {gradient_type} gradient variant\\n-------------')\n",
        "          self.w = self.gradientDescent( X, L, y, w0, alpha, h, tolerance, max_iterations, explicit)\n",
        "          self.all = None\n",
        "        elif gradient_type == 'multipoint':\n",
        "          print(f'------------\\nUsing the {gradient_type} gradient variant\\n-------------')\n",
        "          self.w, _ , self.all = self.run_optimization(num_points, X, L, y, alpha, h, tolerance, max_iterations, explicit = explicit)\n",
        "        elif gradient_type == 'lrSchedual':\n",
        "          print(f'------------\\nUsing the {gradient_type} gradient variant\\n-------------')\n",
        "          self.w, _, self.all = self.gradient_descent_lrSchedual(num_epochs, num_points, X, L, y, alpha, h, tolerance, max_iterations, explicit)\n",
        "\n",
        "        return self.all\n",
        "\n",
        "    # predict the output from testing data\n",
        "    def predict(self, X):\n",
        "        # adding a column in to X matrix as intercept\n",
        "        preds = []\n",
        "        X = np.hstack((np.ones([X.shape[0],1]), X))\n",
        "        if self.all is None:\n",
        "            type(self.all)\n",
        "            preds = X@self.w\n",
        "        else:\n",
        "            for i in self.all:\n",
        "                pred = X@i\n",
        "                preds.append(pred)\n",
        "            preds = np.mean(preds, 0)\n",
        "\n",
        "        return preds\n",
        "\n",
        "    # 2. Gradient Descent with n random points\n",
        "    def run_optimization(self, num_points, X, f, y, alpha, h, tolerance, maxIterations, explicit = True):\n",
        "        best_params = None\n",
        "        best_loss = float('inf')\n",
        "        all_weight = []\n",
        "\n",
        "        for _ in range(num_points):\n",
        "            # Generate a random starting point\n",
        "            initial_theta = np.random.rand(X.shape[1])  # Assuming 2 parameters (x and y)\n",
        "            print('Initial starting points: ', initial_theta )\n",
        "\n",
        "            # Perform gradient descent\n",
        "            optimized_theta = self.gradientDescent(X, f, y, initial_theta, alpha, h, tolerance, maxIterations,  use_explicit_gradient = explicit)\n",
        "\n",
        "            # Calculate the loss for the optimized parameters\n",
        "            current_loss = f(optimized_theta)\n",
        "\n",
        "            # Check if this run has the lowest loss so far\n",
        "            if current_loss < best_loss:\n",
        "                best_loss = current_loss\n",
        "                best_params = optimized_theta\n",
        "\n",
        "            all_weight.append(optimized_theta)\n",
        "\n",
        "        return best_params, best_loss, all_weight\n",
        "\n",
        "    # gradient with epoch\n",
        "    def gradient_descent_lrSchedual(self, num_epochs, num_points, X, f, y, alpha, h, tolerance, maxIterations, explicit):\n",
        "        # Gradient Descent optimization algorithm with cyclic learning rate.\n",
        "        # x = x0\n",
        "        best_params = None\n",
        "        best_loss = float('inf')\n",
        "        all_best_w = []\n",
        "        all_best_loss = []\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            learning_rate = alpha / (epoch + 1) # reduce learning rate for every epoch\n",
        "            print('Epoch ', epoch)\n",
        "\n",
        "            # x, loss = run_optimization(num_points, f, y, alpha, h, tolerance, maxIterations, explicit = True)\n",
        "\n",
        "            x, loss , all_weights = self.run_optimization(num_points, X, f, y, alpha, h, tolerance, maxIterations, explicit = explicit)\n",
        "\n",
        "\n",
        "            # Save parameters every time the learning rate vanishes\n",
        "            np.savetxt(f\"model_parameters_epoch_{epoch}.txt\", x)\n",
        "\n",
        "            # Calculate the loss for the optimized parameters\n",
        "            current_loss = loss\n",
        "\n",
        "            # Check if this run has the lowest loss so far\n",
        "            if current_loss < best_loss:\n",
        "                best_loss = current_loss\n",
        "                best_params = x\n",
        "                best_epoch = epoch\n",
        "        all_best_w.append(best_params)\n",
        "        all_best_loss.append(best_loss)\n",
        "        print('Finding the best parameters at the epoch of ', best_epoch)\n",
        "\n",
        "        return best_params, all_best_loss, all_best_w\n",
        "\n",
        "\n",
        "    # run gradient descent and output the coordinates of the estimated critical point\n",
        "    def gradientDescent(self, X, f, y, x0, alpha, h, tolerance, maxIterations, use_explicit_gradient = False):\n",
        "        # set x equal to the initial guess\n",
        "        x = x0\n",
        "\n",
        "        # take up to maxIterations number of steps\n",
        "        for counter in range(maxIterations):\n",
        "            if use_explicit_gradient:\n",
        "              gradient = self.compute_explicit_gradient(X, y, x)\n",
        "            else:\n",
        "              gradient = self.computeGradient(f, x, h)\n",
        "\n",
        "\n",
        "            # stop if the norm of the gradient is near 0 (success)\n",
        "            if np.linalg.norm(gradient) < tolerance:\n",
        "                print('Gradient descent took', counter, 'iterations to converge')\n",
        "\n",
        "                # print('The norm of the gradient is', np.linalg.norm(gradient))\n",
        "\n",
        "                # return the approximate critical point x\n",
        "                return x\n",
        "\n",
        "            # print a message if we do not converge (failure)\n",
        "            elif counter == maxIterations-1:\n",
        "                print(\"Gradient descent failed\")\n",
        "\n",
        "                # return x, sometimes it is still pretty good\n",
        "                return x\n",
        "\n",
        "            # take a step in the opposite direction as the gradient\n",
        "            x -= alpha*gradient\n",
        "\n",
        "    # estimate the gradient\n",
        "    def computeGradient(self, f, x, h):\n",
        "        n = len(x)\n",
        "        gradient = np.zeros(n)\n",
        "\n",
        "        for counter in range(n):\n",
        "            xUp = x.copy()\n",
        "            xUp[counter] += h\n",
        "            gradient[counter] = (f(xUp) - f(x))/h\n",
        "\n",
        "        return gradient\n",
        "\n",
        "    # Explicit gradient\n",
        "    def compute_explicit_gradient(self, X, y, w):\n",
        "        # Calculate the gradient of the function with respect to w\n",
        "        gradient = 2 * np.dot(X.T, (np.dot(X, w) - y))\n",
        "\n",
        "        return gradient\n"
      ],
      "metadata": {
        "id": "v4qkaLpPbvOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Test model =====> Save all sets of parameters"
      ],
      "metadata": {
        "id": "h8CV0fKo0Cww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[6], [7], [8], [9], [7] ])\n",
        "y = np.array([1, 2, 3, 3, 4])\n",
        "num_epochs = 3\n",
        "num_points = 2 # sets initial values\n",
        "\n",
        "explicits = [True, False]\n",
        "gradient_type = ['multipoint']\n",
        "\n",
        "x0 = np.random.rand(X.shape[1] +1)*0.001\n",
        "for explicit in explicits:\n",
        "  if explicit: print('\\nUsing explicit gradient\\n-------------------------------------------------------------')\n",
        "  else: print('\\nUsing numerical gradient method\\n-------------------------------------------------------------')\n",
        "\n",
        "  for g_type in gradient_type:\n",
        "    model = Model()\n",
        "    model.fit(num_epochs, num_points, X, y, x0, alpha = 0.001, h = 0.001, tolerance = 0.01, max_iterations = 10000000, gradient_type = g_type, explicit = False)\n",
        "    prediction = model.predict(X)\n",
        "\n",
        "    # print('Prediction from exact solution: \\n', prediction)\n",
        "    print('Mean of predictions: ', prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3fGwgsN0NYp",
        "outputId": "ff0753ad-8123-405b-f0ca-31243474a50e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using explicit gradient\n",
            "-------------------------------------------------------------\n",
            "------------\n",
            "Using the multipoint gradient variant\n",
            "-------------\n",
            "Initial starting points:  [0.21266287 0.19583833]\n",
            "Gradient descent took 17923 iterations to converge\n",
            "Initial starting points:  [0.98371829 0.14897675]\n",
            "Gradient descent took 20243 iterations to converge\n",
            "Mean of predictions:  [1.88927257 2.39728586 2.90529915 3.41331243 2.39728586]\n",
            "\n",
            "Using numerical gradient method\n",
            "-------------------------------------------------------------\n",
            "------------\n",
            "Using the multipoint gradient variant\n",
            "-------------\n",
            "Initial starting points:  [0.14544756 0.6303754 ]\n",
            "Gradient descent took 17438 iterations to converge\n",
            "Initial starting points:  [0.57384009 0.82776784]\n",
            "Gradient descent took 18868 iterations to converge\n",
            "Mean of predictions:  [1.88927218 2.39728572 2.90529926 3.4133128  2.39728572]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Test model on [Mount Pleasant Real Estate Dataset](https://www.hawkeslearning.com/Statistics/dis/datasets.html)\n"
      ],
      "metadata": {
        "id": "mIJwWPC9a_Kh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Testing six variants of the gradient\n",
        "\n",
        "Before jumping on the Mount pleasant Real Estate dataset, it should be better to test it on a samll dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "DkKNLtUm1Ogu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[6], [7], [8], [9], [7] ])\n",
        "y = np.array([1, 2, 3, 3, 4])\n",
        "num_epochs = 2\n",
        "num_points = 2 # sets initial values\n",
        "\n",
        "explicits = [True, False]\n",
        "gradient_type = ['simple', 'multipoint', 'lrSchedual']\n",
        "\n",
        "x0 = np.random.rand(X.shape[1] +1)*0.001\n",
        "for explicit in explicits:\n",
        "  for g_type in gradient_type:\n",
        "    model = Model()\n",
        "    model.fit(num_epochs, num_points, X, y, x0, alpha = 0.001, h = 0.001, tolerance = 0.01, max_iterations = 10000000, gradient_type = g_type, explicit = False)\n",
        "    prediction = model.predict(X)\n",
        "\n",
        "    # print('Prediction from exact solution: \\n', prediction)\n",
        "    print('Mean of predictions: ', prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb_s_cVffbTg",
        "outputId": "8fda335d-0567-4863-d5a7-5c984599886f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------\n",
            "Using the simple gradient variant\n",
            "-------------\n",
            "Gradient descent took 17182 iterations to converge\n",
            "Mean of predictions:  [1.88927226 2.39728575 2.90529924 3.41331273 2.39728575]\n",
            "------------\n",
            "Using the multipoint gradient variant\n",
            "-------------\n",
            "Initial starting points:  [0.07109899 0.3307451 ]\n",
            "Gradient descent took 17295 iterations to converge\n",
            "Initial starting points:  [0.87212846 0.493542  ]\n",
            "Gradient descent took 19846 iterations to converge\n",
            "Mean of predictions:  [1.88927326 2.3972861  2.90529894 3.41331178 2.3972861 ]\n",
            "------------\n",
            "Using the lrSchedual gradient variant\n",
            "-------------\n",
            "Epoch  0\n",
            "Initial starting points:  [0.12730143 0.19863234]\n",
            "Gradient descent took 17594 iterations to converge\n",
            "Initial starting points:  [0.52929182 0.21407131]\n",
            "Gradient descent took 18982 iterations to converge\n",
            "Epoch  1\n",
            "Initial starting points:  [0.92998809 0.3062563 ]\n",
            "Gradient descent took 20058 iterations to converge\n",
            "Initial starting points:  [0.39005756 0.34445057]\n",
            "Gradient descent took 18480 iterations to converge\n",
            "Finding the best parameters at the epoch of  0\n",
            "Mean of predictions:  [1.88927247 2.39728583 2.90529918 3.41331253 2.39728583]\n",
            "------------\n",
            "Using the simple gradient variant\n",
            "-------------\n",
            "Gradient descent took 0 iterations to converge\n",
            "Mean of predictions:  [1.88927226 2.39728575 2.90529924 3.41331273 2.39728575]\n",
            "------------\n",
            "Using the multipoint gradient variant\n",
            "-------------\n",
            "Initial starting points:  [0.35224786 0.06372644]\n",
            "Gradient descent took 18478 iterations to converge\n",
            "Initial starting points:  [0.57433114 0.21503871]\n",
            "Gradient descent took 19118 iterations to converge\n",
            "Mean of predictions:  [1.88927256 2.39728586 2.90529915 3.41331245 2.39728586]\n",
            "------------\n",
            "Using the lrSchedual gradient variant\n",
            "-------------\n",
            "Epoch  0\n",
            "Initial starting points:  [0.8353487  0.25467736]\n",
            "Gradient descent took 19833 iterations to converge\n",
            "Initial starting points:  [8.53375287e-01 3.40840943e-04]\n",
            "Gradient descent took 19967 iterations to converge\n",
            "Epoch  1\n",
            "Initial starting points:  [0.1312203  0.16558042]\n",
            "Gradient descent took 17627 iterations to converge\n",
            "Initial starting points:  [0.26628505 0.06858996]\n",
            "Gradient descent took 18179 iterations to converge\n",
            "Finding the best parameters at the epoch of  0\n",
            "Mean of predictions:  [1.88927269 2.3972859  2.90529911 3.41331232 2.3972859 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Prediction the Mount_Pleasant_Real_Estate_Data"
      ],
      "metadata": {
        "id": "9T13g_c_1XgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "df = pd.read_csv('/content/Mount_Pleasant_Real_Estate_Data.csv')\n",
        "\n",
        "# Define a mapping from 'yes' to 1 and 'no' to 0\n",
        "binary_mapping = {'Yes': 1, 'No': 0}\n",
        "\n",
        "# Convert the 'yes/no' column to binary\n",
        "df['Duplex?'] = df['Duplex?'].map(binary_mapping) # C\n",
        "df['New Owned?'] = df['New Owned?'].map(binary_mapping) # M\n",
        "\n",
        "df['Has Pool?'] = df['Has Pool?'].map(binary_mapping) # Q\n",
        "df['Has Dock?'] = df['Has Dock?'].map(binary_mapping) # R\n",
        "df['Fenced Yard'] = df['Fenced Yard'].map(binary_mapping) # S\n",
        "df['Screened Porch?'] = df['Screened Porch?'].map(binary_mapping) # T\n",
        "\n",
        "df['Golf Course?'] = df['Golf Course?'].map(binary_mapping) # V\n",
        "df['Fireplace?'] = df['Fireplace?'].map(binary_mapping) # W\n",
        "\n",
        "categorical_columns = ['Subdivision', 'House Style', ]\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Apply label encoding to each categorical column\n",
        "for col in categorical_columns:\n",
        "    df[col] = label_encoder.fit_transform(df[col])\n",
        "\n",
        "# Extracting input and ground truth from the data\n",
        "input_columns = df.columns[list(range(2, 14)) + list(range(16, 19)) + list(range(21, 22))]\n",
        "\n",
        "# Create a new DataFrame containing only the selected input columns\n",
        "X = np.array(df[input_columns], dtype=float)\n",
        "y = np.array(df[df.columns[1]], dtype=float)\n",
        "\n",
        "# Using data from the defined columns (C-O, Q-T, and V-W)\n",
        "print(X.shape, np.mean(X, axis = 0).shape, np.mean(X, axis = 1).shape)\n",
        "\n",
        "#Normalize data with some columns\n",
        "X[:, [1, 2, 3, 5, 7, 8, 9]] = (X[:, [1, 2, 3, 5, 7, 8, 9]] - np.mean(X[:, [1, 2, 3, 5, 7, 8, 9]], axis = 0))/np.std(X[:, [1, 2, 3, 5, 7, 8, 9]], axis = 0)\n",
        "\n",
        "# split the data into training and test sets\n",
        "trainX, testX, trainY, testY = train_test_split(X, y, train_size = 0.24, random_state =42)\n",
        "\n",
        "trainX = scale(trainX)\n",
        "testX = scale(testX)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSvTnYnWJ9-a",
        "outputId": "f8e28c87-a2be-468a-bba8-7c63b9db6224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(245, 16) (16,) (245,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training model and and using saved weights for prediction"
      ],
      "metadata": {
        "id": "BcuYdqn16S43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "explicits = [True, False]\n",
        "gradient_type = ['simple', 'multipoint', 'lrSchedual']\n",
        "num_epochs = 2\n",
        "num_points = 2 # sets initial values\n",
        "x0 = np.random.rand(trainX.shape[1] +1)*0.001\n",
        "for explicit in explicits:\n",
        "  if explicit: print('\\nUsing explicit gradient\\n-------------------------------------------------------------')\n",
        "  else: print('\\nUsing numerical gradient method\\n-------------------------------------------------------------')\n",
        "\n",
        "  for g_type in gradient_type:\n",
        "    model = Model()\n",
        "\n",
        "    # Fitting model\n",
        "    fstart = time.time()\n",
        "    model.fit(num_epochs, num_points,  trainX, trainY, x0, alpha = 0.001, h = 0.001, tolerance = 0.01, max_iterations = 10000, gradient_type = g_type, explicit = False)\n",
        "    fend = time.time()\n",
        "    print(f\"\\nFitting time is {fend - fstart}\")\n",
        "\n",
        "    print('\\nPredicting the model...\\n')\n",
        "\n",
        "    trainPredictions_num_all = model.predict(X)\n",
        "\n",
        "    # Prediction\n",
        "    pstart = time.time()\n",
        "    trainPredictions_num = model.predict(trainX)\n",
        "    pend = time.time()\n",
        "    print(f\"Prediction time is {pend - pstart}\")\n",
        "\n",
        "    print('\\nThe r^2 score is\\t', r2_score(trainY, trainPredictions_num))\n",
        "\n",
        "\n",
        "    # print quality metrics\n",
        "    print('\\nThe mean absolute error on the training set is\\t', mean_absolute_error(trainY, trainPredictions_num))\n",
        "\n",
        "    # return the predicted outputs for the datapoints in the test set\n",
        "    predictions_num = model.predict(testX)\n",
        "\n",
        "    # print the predictions\n",
        "    print('\\nThe predicted y values for the test set are\\t', np.round(predictions_num,0))\n",
        "\n",
        "    # print the real y values\n",
        "    print('The real y values for the test set are\\t', testY)\n",
        "\n",
        "    # print the weights\n",
        "    print('\\nThe weights are\\t', model.w)\n",
        "\n",
        "    # print quality metrics\n",
        "    print('\\nThe mean absolute error on the test set is\\t', mean_absolute_error(testY, predictions_num), '\\n')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLSnUc012IXr",
        "outputId": "faed36b2-7223-4c51-9a97-d0a6bfa01bfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using explicit gradient\n",
            "-------------------------------------------------------------\n",
            "------------\n",
            "Using the simple gradient variant\n",
            "-------------\n",
            "Gradient descent failed\n",
            "\n",
            "Fitting time is 4.387463569641113\n",
            "\n",
            "Predicting the model...\n",
            "\n",
            "Prediction time is 5.507469177246094e-05\n",
            "\n",
            "The r^2 score is\t 0.9382950520219118\n",
            "\n",
            "The mean absolute error on the training set is\t 56305.72730190991\n",
            "\n",
            "The predicted y values for the test set are\t [ 494426.  726599.  194054.  660816.  614813.  502575. 1008868.  226799.  586574.  323530.  729905.  503266.  350834.  112098.  677598. 1381571. 1049112.  549131.  604425.\n",
            "  675969.  670583.  697544. 2013777.  566526.  773956. 1131719.  637799.  795734.  504825.  487537. 1247219.  436431.  550857.  773943.  524459.  271180.  442128.  706538.\n",
            " 1332974. 1012579.  649839.  278532. 1285201.  514297.  953049.  709082.  597628.  684080.  530024.  509711.  659097.  455158.  557709.  566622.  915671.  595047.  224909.\n",
            "  529318.  452727.  307972.  293040.  367541.  473323.  333771.  806697.  297080.  694482. 1394239.  453489.  492269.  135957. 1170284.  777386.  622023.  239845.  398314.\n",
            "  449971.  482928. 1162346. 1190989. 1173324. 1224957.  773417.  724045.  594633.  491210.  631106.  553471.  671598.  614814.  695349. 1372717.  276833.  381843.  461402.\n",
            "  454062.  365645.  800408.  916158.  939110.  714326. 1665832.  736605.  720753.  687184.  700942.  400234.  285561.  593599.  720712.  333643.  670916.  802636.  725724.\n",
            "  337850.  715171.  922488.  502259.  453612.  634687.  173067.  471849.  511818.  556377. 1557089.  707246.  600749.  310494.  381507.  657649.  574754. 1256464.  586906.\n",
            "  174735.  483501.  234605.  551547.  856864.  311098.  655498.  683441.  537205.  618366.  549505.  608391.  578913.  314947.  930801.  710887.  476759.  647750.  310473.\n",
            "  575762.  466005. 1053375.  754709.  582837.  712407.  731955.  425297.  211258.  334866.  635816. 1378116.  283457.  748924.  351066.  505643.  685595.  456594.  846733.\n",
            "  571802.  505643.  194054.  573950.  575591.  275888.  713792.  450751. 1390340.  597226.  763411.  533076.  437059.  552105.  491604.  548640.]\n",
            "The real y values for the test set are\t [ 525000.  575000.  179900.  599000.  795000.  503990.  875000.  232900.  469900.  349900.  725900.  600000.  425000.  226900.  649900. 1112885.  999999.  505000.  756500.\n",
            "  879000.  569900.  512000. 1699000.  561900.  689950. 1199000.  569900.  715000.  426990.  416990. 1375000.  497900.  529900.  875000.  509000.  237000.  440000.  840000.\n",
            "  827655. 1399000.  699900.  245000. 1345000.  505900.  850000.  750000.  689000.  589900.  639000.  421999.  650000.  445000.  456990.  630000.  975000.  564775.  341500.\n",
            "  505990.  406990.  359500.  349999.  369000.  508990.  450000.  739000.  329900.  735000. 1500000.  474990.  500000.  124900.  899000.  665000.  549900.  327000.  429000.\n",
            "  508000.  446900.  925000. 1050000. 1450000. 1199000.  795000.  750000.  659900.  479900.  564900.  478900.  574900.  534900.  613290. 1800000.  362500.  469900.  439990.\n",
            "  472900.  445000.  832140.  868000.  698980.  609900. 1818000.  874900.  669900.  935000.  530000.  325000.  339000.  511000.  875000.  369900.  769900.  698000.  729900.\n",
            "  389900.  774650. 1136900.  486990.  529000.  617500.  205900.  437990.  430000.  574900.  891846.  875000.  522000.  394000.  425000.  765135.  530000. 1595000.  534000.\n",
            "  119900.  436625.  224900.  564900.  840000.  369900.  827189.  840000.  763634.  615000.  854437.  511900.  735000.  319500.  799900.  651948.  490000.  599900.  229000.\n",
            "  522075.  509990. 1250000.  650000.  589900.  720000.  775000.  445000.  220000.  347000.  614900. 1005685.  360000.  699990.  324000.  477771.  750000.  465000.  785000.\n",
            "  930525.  450000.  189500.  655000.  571900.  364900.  575000.  475990. 1575000.  523900.  635000.  499990.  464500.  540000.  464900.  478900.]\n",
            "\n",
            "The weights are\t [634761.25816247 -34494.86466404 -81610.5900283   10255.42106726  15948.48912649 -16568.23366509 -10428.47516992 -36557.31317473 202954.09683268  29300.70175627 105893.23955985\n",
            "   2361.25025984 -18553.07889897  14100.13473704 100016.74244293  23103.78044701 -30947.09450346]\n",
            "\n",
            "The mean absolute error on the test set is\t 88908.1552379255 \n",
            "\n",
            "------------\n",
            "Using the multipoint gradient variant\n",
            "-------------\n",
            "Initial starting points:  [0.43317052 0.33368267 0.33373494 0.58121629 0.10016604 0.72934512 0.00960239 0.190162   0.98312608 0.92215964 0.86181611 0.82193139 0.70759383 0.52481576 0.48358516 0.02135055\n",
            " 0.2156295 ]\n",
            "Gradient descent failed\n",
            "Initial starting points:  [0.96569969 0.10609319 0.81450569 0.57004678 0.97977063 0.47257296 0.56403091 0.93224765 0.49381016 0.98757366 0.98703517 0.00595227 0.91731649 0.1406876  0.30068789 0.26916411\n",
            " 0.6031078 ]\n",
            "Gradient descent failed\n",
            "\n",
            "Fitting time is 8.090279817581177\n",
            "\n",
            "Predicting the model...\n",
            "\n",
            "Prediction time is 8.416175842285156e-05\n",
            "\n",
            "The r^2 score is\t 0.9382950520219118\n",
            "\n",
            "The mean absolute error on the training set is\t 56305.727359963355\n",
            "\n",
            "The predicted y values for the test set are\t [ 494426.  726599.  194054.  660816.  614813.  502575. 1008868.  226799.  586574.  323530.  729905.  503266.  350834.  112098.  677598. 1381571. 1049112.  549131.  604425.\n",
            "  675969.  670583.  697544. 2013777.  566526.  773956. 1131719.  637799.  795733.  504825.  487537. 1247219.  436431.  550857.  773943.  524459.  271180.  442128.  706538.\n",
            " 1332974. 1012579.  649839.  278532. 1285201.  514297.  953049.  709082.  597628.  684080.  530024.  509711.  659097.  455158.  557709.  566622.  915671.  595047.  224909.\n",
            "  529318.  452727.  307972.  293040.  367541.  473323.  333771.  806697.  297080.  694482. 1394239.  453489.  492269.  135957. 1170284.  777386.  622023.  239845.  398314.\n",
            "  449971.  482928. 1162346. 1190989. 1173324. 1224957.  773417.  724045.  594633.  491210.  631106.  553471.  671598.  614814.  695349. 1372717.  276833.  381843.  461402.\n",
            "  454062.  365645.  800408.  916158.  939110.  714326. 1665832.  736605.  720753.  687184.  700942.  400234.  285561.  593599.  720712.  333643.  670916.  802636.  725724.\n",
            "  337850.  715171.  922488.  502259.  453612.  634687.  173067.  471849.  511818.  556377. 1557089.  707246.  600749.  310494.  381507.  657649.  574754. 1256464.  586906.\n",
            "  174735.  483501.  234605.  551547.  856864.  311098.  655498.  683441.  537205.  618366.  549505.  608391.  578913.  314947.  930801.  710887.  476759.  647750.  310473.\n",
            "  575762.  466005. 1053375.  754709.  582837.  712407.  731955.  425297.  211258.  334866.  635816. 1378116.  283457.  748924.  351066.  505643.  685595.  456594.  846733.\n",
            "  571802.  505643.  194054.  573950.  575591.  275888.  713792.  450751. 1390340.  597226.  763411.  533076.  437059.  552105.  491604.  548640.]\n",
            "The real y values for the test set are\t [ 525000.  575000.  179900.  599000.  795000.  503990.  875000.  232900.  469900.  349900.  725900.  600000.  425000.  226900.  649900. 1112885.  999999.  505000.  756500.\n",
            "  879000.  569900.  512000. 1699000.  561900.  689950. 1199000.  569900.  715000.  426990.  416990. 1375000.  497900.  529900.  875000.  509000.  237000.  440000.  840000.\n",
            "  827655. 1399000.  699900.  245000. 1345000.  505900.  850000.  750000.  689000.  589900.  639000.  421999.  650000.  445000.  456990.  630000.  975000.  564775.  341500.\n",
            "  505990.  406990.  359500.  349999.  369000.  508990.  450000.  739000.  329900.  735000. 1500000.  474990.  500000.  124900.  899000.  665000.  549900.  327000.  429000.\n",
            "  508000.  446900.  925000. 1050000. 1450000. 1199000.  795000.  750000.  659900.  479900.  564900.  478900.  574900.  534900.  613290. 1800000.  362500.  469900.  439990.\n",
            "  472900.  445000.  832140.  868000.  698980.  609900. 1818000.  874900.  669900.  935000.  530000.  325000.  339000.  511000.  875000.  369900.  769900.  698000.  729900.\n",
            "  389900.  774650. 1136900.  486990.  529000.  617500.  205900.  437990.  430000.  574900.  891846.  875000.  522000.  394000.  425000.  765135.  530000. 1595000.  534000.\n",
            "  119900.  436625.  224900.  564900.  840000.  369900.  827189.  840000.  763634.  615000.  854437.  511900.  735000.  319500.  799900.  651948.  490000.  599900.  229000.\n",
            "  522075.  509990. 1250000.  650000.  589900.  720000.  775000.  445000.  220000.  347000.  614900. 1005685.  360000.  699990.  324000.  477771.  750000.  465000.  785000.\n",
            "  930525.  450000.  189500.  655000.  571900.  364900.  575000.  475990. 1575000.  523900.  635000.  499990.  464500.  540000.  464900.  478900.]\n",
            "\n",
            "The weights are\t [634761.25867101 -34494.8641933  -81610.58978801  10255.55576463  15948.36072512 -16568.27346249 -10428.47471158 -36557.31307286 202954.09616319  29300.70286033 105893.2395627\n",
            "   2361.25003198 -18553.07890519  14100.13474008 100016.74341182  23103.78062789 -30947.09449013]\n",
            "\n",
            "The mean absolute error on the test set is\t 88908.15517051792 \n",
            "\n",
            "------------\n",
            "Using the lrSchedual gradient variant\n",
            "-------------\n",
            "Epoch  0\n",
            "Initial starting points:  [0.647985   0.68008024 0.74189451 0.85640445 0.92156555 0.49063376 0.06020246 0.97736363 0.58930201 0.2476778  0.97792969 0.60379852 0.98771683 0.17123116 0.60860725 0.44564649\n",
            " 0.92897805]\n",
            "Gradient descent failed\n",
            "Initial starting points:  [0.97500676 0.03148691 0.13020956 0.96178571 0.05797459 0.99366714 0.67471735 0.05695176 0.03593704 0.33006101 0.83547247 0.36280556 0.8331374  0.12352727 0.69871888 0.97574228\n",
            " 0.30958939]\n",
            "Gradient descent failed\n",
            "Epoch  1\n",
            "Initial starting points:  [0.70335146 0.10516564 0.0013209  0.78158397 0.61270419 0.96553422 0.91815062 0.05340935 0.63932085 0.03053687 0.27207363 0.58498847 0.63755245 0.05782239 0.08502229 0.99381005\n",
            " 0.37693246]\n",
            "Gradient descent failed\n",
            "Initial starting points:  [0.55991034 0.43609889 0.27931737 0.68023384 0.81953621 0.27945402 0.7429766  0.2008475  0.93325902 0.35825359 0.35435008 0.92026697 0.4422403  0.28226809 0.72116011 0.13178025\n",
            " 0.78970071]\n",
            "Gradient descent failed\n",
            "Finding the best parameters at the epoch of  0\n",
            "\n",
            "Fitting time is 17.561971426010132\n",
            "\n",
            "Predicting the model...\n",
            "\n",
            "Prediction time is 9.751319885253906e-05\n",
            "\n",
            "The r^2 score is\t 0.9382950520219119\n",
            "\n",
            "The mean absolute error on the training set is\t 56305.72748290259\n",
            "\n",
            "The predicted y values for the test set are\t [ 494426.  726599.  194054.  660816.  614813.  502575. 1008868.  226799.  586574.  323530.  729905.  503266.  350834.  112098.  677598. 1381571. 1049112.  549131.  604425.\n",
            "  675969.  670583.  697544. 2013777.  566526.  773956. 1131719.  637799.  795734.  504825.  487537. 1247219.  436431.  550857.  773943.  524459.  271180.  442128.  706538.\n",
            " 1332974. 1012579.  649839.  278532. 1285201.  514297.  953049.  709082.  597628.  684080.  530024.  509711.  659097.  455158.  557709.  566622.  915671.  595047.  224909.\n",
            "  529318.  452727.  307972.  293040.  367541.  473323.  333771.  806697.  297080.  694482. 1394239.  453489.  492269.  135957. 1170284.  777386.  622023.  239845.  398314.\n",
            "  449971.  482928. 1162346. 1190989. 1173324. 1224957.  773417.  724045.  594633.  491210.  631106.  553471.  671598.  614814.  695349. 1372717.  276833.  381843.  461402.\n",
            "  454062.  365645.  800408.  916158.  939110.  714326. 1665832.  736605.  720753.  687184.  700942.  400234.  285561.  593599.  720712.  333643.  670916.  802636.  725724.\n",
            "  337850.  715171.  922488.  502259.  453612.  634687.  173067.  471849.  511818.  556377. 1557089.  707246.  600749.  310494.  381507.  657649.  574754. 1256464.  586906.\n",
            "  174735.  483501.  234605.  551547.  856864.  311098.  655498.  683441.  537205.  618366.  549505.  608391.  578913.  314947.  930801.  710887.  476759.  647750.  310473.\n",
            "  575762.  466005. 1053375.  754709.  582837.  712407.  731955.  425297.  211258.  334866.  635816. 1378116.  283457.  748924.  351066.  505643.  685595.  456594.  846733.\n",
            "  571802.  505643.  194054.  573950.  575591.  275888.  713792.  450751. 1390340.  597226.  763411.  533076.  437059.  552105.  491604.  548640.]\n",
            "The real y values for the test set are\t [ 525000.  575000.  179900.  599000.  795000.  503990.  875000.  232900.  469900.  349900.  725900.  600000.  425000.  226900.  649900. 1112885.  999999.  505000.  756500.\n",
            "  879000.  569900.  512000. 1699000.  561900.  689950. 1199000.  569900.  715000.  426990.  416990. 1375000.  497900.  529900.  875000.  509000.  237000.  440000.  840000.\n",
            "  827655. 1399000.  699900.  245000. 1345000.  505900.  850000.  750000.  689000.  589900.  639000.  421999.  650000.  445000.  456990.  630000.  975000.  564775.  341500.\n",
            "  505990.  406990.  359500.  349999.  369000.  508990.  450000.  739000.  329900.  735000. 1500000.  474990.  500000.  124900.  899000.  665000.  549900.  327000.  429000.\n",
            "  508000.  446900.  925000. 1050000. 1450000. 1199000.  795000.  750000.  659900.  479900.  564900.  478900.  574900.  534900.  613290. 1800000.  362500.  469900.  439990.\n",
            "  472900.  445000.  832140.  868000.  698980.  609900. 1818000.  874900.  669900.  935000.  530000.  325000.  339000.  511000.  875000.  369900.  769900.  698000.  729900.\n",
            "  389900.  774650. 1136900.  486990.  529000.  617500.  205900.  437990.  430000.  574900.  891846.  875000.  522000.  394000.  425000.  765135.  530000. 1595000.  534000.\n",
            "  119900.  436625.  224900.  564900.  840000.  369900.  827189.  840000.  763634.  615000.  854437.  511900.  735000.  319500.  799900.  651948.  490000.  599900.  229000.\n",
            "  522075.  509990. 1250000.  650000.  589900.  720000.  775000.  445000.  220000.  347000.  614900. 1005685.  360000.  699990.  324000.  477771.  750000.  465000.  785000.\n",
            "  930525.  450000.  189500.  655000.  571900.  364900.  575000.  475990. 1575000.  523900.  635000.  499990.  464500.  540000.  464900.  478900.]\n",
            "\n",
            "The weights are\t [634761.25845863 -34494.8635599  -81610.58897707  10255.3467609   15948.56011535 -16568.21127053 -10428.47501483 -36557.31310268 202954.09619899  29300.70238972 105893.24038086\n",
            "   2361.25077118 -18553.07868942  14100.13473213 100016.7432508   23103.78127881 -30947.0950698 ]\n",
            "\n",
            "The mean absolute error on the test set is\t 88908.15505923434 \n",
            "\n",
            "\n",
            "Using numerical gradient method\n",
            "-------------------------------------------------------------\n",
            "------------\n",
            "Using the simple gradient variant\n",
            "-------------\n",
            "Gradient descent failed\n",
            "\n",
            "Fitting time is 3.8235373497009277\n",
            "\n",
            "Predicting the model...\n",
            "\n",
            "Prediction time is 5.91278076171875e-05\n",
            "\n",
            "The r^2 score is\t 0.9382950520219118\n",
            "\n",
            "The mean absolute error on the training set is\t 56305.72718656694\n",
            "\n",
            "The predicted y values for the test set are\t [ 494426.  726599.  194054.  660816.  614813.  502575. 1008868.  226799.  586574.  323530.  729905.  503266.  350834.  112098.  677598. 1381571. 1049112.  549131.  604425.\n",
            "  675969.  670583.  697544. 2013777.  566526.  773956. 1131719.  637799.  795734.  504825.  487537. 1247219.  436431.  550857.  773943.  524459.  271180.  442128.  706538.\n",
            " 1332974. 1012579.  649839.  278532. 1285201.  514297.  953049.  709082.  597628.  684080.  530024.  509711.  659097.  455158.  557709.  566622.  915671.  595047.  224909.\n",
            "  529318.  452727.  307972.  293040.  367541.  473323.  333771.  806697.  297080.  694482. 1394239.  453489.  492269.  135957. 1170284.  777386.  622023.  239845.  398314.\n",
            "  449971.  482928. 1162346. 1190989. 1173324. 1224957.  773417.  724045.  594633.  491210.  631106.  553471.  671598.  614814.  695349. 1372717.  276833.  381843.  461402.\n",
            "  454062.  365645.  800408.  916158.  939110.  714326. 1665832.  736605.  720753.  687184.  700942.  400234.  285561.  593599.  720712.  333643.  670916.  802636.  725724.\n",
            "  337850.  715171.  922488.  502259.  453612.  634687.  173067.  471849.  511818.  556377. 1557089.  707246.  600749.  310494.  381507.  657649.  574754. 1256464.  586906.\n",
            "  174735.  483501.  234605.  551547.  856864.  311098.  655498.  683441.  537205.  618366.  549505.  608391.  578913.  314947.  930801.  710887.  476759.  647750.  310473.\n",
            "  575762.  466005. 1053375.  754709.  582837.  712407.  731955.  425297.  211258.  334866.  635816. 1378116.  283457.  748924.  351066.  505643.  685595.  456594.  846733.\n",
            "  571802.  505643.  194054.  573950.  575591.  275888.  713792.  450751. 1390340.  597226.  763411.  533076.  437059.  552105.  491604.  548640.]\n",
            "The real y values for the test set are\t [ 525000.  575000.  179900.  599000.  795000.  503990.  875000.  232900.  469900.  349900.  725900.  600000.  425000.  226900.  649900. 1112885.  999999.  505000.  756500.\n",
            "  879000.  569900.  512000. 1699000.  561900.  689950. 1199000.  569900.  715000.  426990.  416990. 1375000.  497900.  529900.  875000.  509000.  237000.  440000.  840000.\n",
            "  827655. 1399000.  699900.  245000. 1345000.  505900.  850000.  750000.  689000.  589900.  639000.  421999.  650000.  445000.  456990.  630000.  975000.  564775.  341500.\n",
            "  505990.  406990.  359500.  349999.  369000.  508990.  450000.  739000.  329900.  735000. 1500000.  474990.  500000.  124900.  899000.  665000.  549900.  327000.  429000.\n",
            "  508000.  446900.  925000. 1050000. 1450000. 1199000.  795000.  750000.  659900.  479900.  564900.  478900.  574900.  534900.  613290. 1800000.  362500.  469900.  439990.\n",
            "  472900.  445000.  832140.  868000.  698980.  609900. 1818000.  874900.  669900.  935000.  530000.  325000.  339000.  511000.  875000.  369900.  769900.  698000.  729900.\n",
            "  389900.  774650. 1136900.  486990.  529000.  617500.  205900.  437990.  430000.  574900.  891846.  875000.  522000.  394000.  425000.  765135.  530000. 1595000.  534000.\n",
            "  119900.  436625.  224900.  564900.  840000.  369900.  827189.  840000.  763634.  615000.  854437.  511900.  735000.  319500.  799900.  651948.  490000.  599900.  229000.\n",
            "  522075.  509990. 1250000.  650000.  589900.  720000.  775000.  445000.  220000.  347000.  614900. 1005685.  360000.  699990.  324000.  477771.  750000.  465000.  785000.\n",
            "  930525.  450000.  189500.  655000.  571900.  364900.  575000.  475990. 1575000.  523900.  635000.  499990.  464500.  540000.  464900.  478900.]\n",
            "\n",
            "The weights are\t [634761.25797936 -34494.86472508 -81610.59045555  10255.38609412  15948.52257375 -16568.22316704 -10428.47492578 -36557.31354094 202954.09640543  29300.7016342  105893.23901053\n",
            "   2361.2498326  -18553.07969243  14100.13455394 100016.74305328  23103.77989769 -30947.0947476 ]\n",
            "\n",
            "The mean absolute error on the test set is\t 88908.15559131243 \n",
            "\n",
            "------------\n",
            "Using the multipoint gradient variant\n",
            "-------------\n",
            "Initial starting points:  [0.41116166 0.39124646 0.73768532 0.01541953 0.81209592 0.62444997 0.77343965 0.83405562 0.69651895 0.55635725 0.30315471 0.2198949  0.40024014 0.92656787 0.55696142 0.75839137\n",
            " 0.82430583]\n",
            "Gradient descent failed\n",
            "Initial starting points:  [0.17029459 0.81116391 0.7943055  0.39115479 0.39597524 0.36350501 0.38334623 0.60677911 0.3075641  0.51224343 0.63763403 0.89542012 0.51071047 0.72011836 0.54431163 0.53814325\n",
            " 0.76207073]\n",
            "Gradient descent failed\n",
            "\n",
            "Fitting time is 9.115677833557129\n",
            "\n",
            "Predicting the model...\n",
            "\n",
            "Prediction time is 0.00010919570922851562\n",
            "\n",
            "The r^2 score is\t 0.9382950520219118\n",
            "\n",
            "The mean absolute error on the training set is\t 56305.72727769934\n",
            "\n",
            "The predicted y values for the test set are\t [ 494426.  726599.  194054.  660816.  614813.  502575. 1008868.  226799.  586574.  323530.  729905.  503266.  350834.  112098.  677598. 1381571. 1049112.  549131.  604425.\n",
            "  675969.  670583.  697544. 2013777.  566526.  773956. 1131719.  637799.  795734.  504825.  487537. 1247219.  436431.  550857.  773943.  524459.  271180.  442128.  706538.\n",
            " 1332974. 1012579.  649839.  278532. 1285201.  514297.  953049.  709082.  597628.  684080.  530024.  509711.  659097.  455158.  557709.  566622.  915671.  595047.  224909.\n",
            "  529318.  452727.  307972.  293040.  367541.  473323.  333771.  806697.  297080.  694482. 1394239.  453489.  492269.  135957. 1170284.  777386.  622023.  239845.  398314.\n",
            "  449971.  482928. 1162346. 1190989. 1173324. 1224957.  773417.  724045.  594633.  491210.  631106.  553471.  671598.  614814.  695349. 1372717.  276833.  381843.  461403.\n",
            "  454062.  365645.  800408.  916158.  939110.  714326. 1665832.  736605.  720753.  687184.  700942.  400234.  285561.  593599.  720712.  333643.  670916.  802636.  725724.\n",
            "  337850.  715171.  922488.  502259.  453612.  634687.  173067.  471849.  511818.  556377. 1557089.  707246.  600749.  310494.  381507.  657649.  574754. 1256464.  586906.\n",
            "  174735.  483501.  234605.  551547.  856864.  311098.  655498.  683441.  537205.  618366.  549505.  608391.  578913.  314947.  930801.  710887.  476759.  647750.  310473.\n",
            "  575762.  466005. 1053375.  754709.  582837.  712407.  731955.  425297.  211258.  334866.  635816. 1378116.  283457.  748924.  351066.  505643.  685595.  456594.  846733.\n",
            "  571802.  505643.  194054.  573950.  575591.  275888.  713792.  450751. 1390340.  597226.  763411.  533076.  437059.  552105.  491604.  548640.]\n",
            "The real y values for the test set are\t [ 525000.  575000.  179900.  599000.  795000.  503990.  875000.  232900.  469900.  349900.  725900.  600000.  425000.  226900.  649900. 1112885.  999999.  505000.  756500.\n",
            "  879000.  569900.  512000. 1699000.  561900.  689950. 1199000.  569900.  715000.  426990.  416990. 1375000.  497900.  529900.  875000.  509000.  237000.  440000.  840000.\n",
            "  827655. 1399000.  699900.  245000. 1345000.  505900.  850000.  750000.  689000.  589900.  639000.  421999.  650000.  445000.  456990.  630000.  975000.  564775.  341500.\n",
            "  505990.  406990.  359500.  349999.  369000.  508990.  450000.  739000.  329900.  735000. 1500000.  474990.  500000.  124900.  899000.  665000.  549900.  327000.  429000.\n",
            "  508000.  446900.  925000. 1050000. 1450000. 1199000.  795000.  750000.  659900.  479900.  564900.  478900.  574900.  534900.  613290. 1800000.  362500.  469900.  439990.\n",
            "  472900.  445000.  832140.  868000.  698980.  609900. 1818000.  874900.  669900.  935000.  530000.  325000.  339000.  511000.  875000.  369900.  769900.  698000.  729900.\n",
            "  389900.  774650. 1136900.  486990.  529000.  617500.  205900.  437990.  430000.  574900.  891846.  875000.  522000.  394000.  425000.  765135.  530000. 1595000.  534000.\n",
            "  119900.  436625.  224900.  564900.  840000.  369900.  827189.  840000.  763634.  615000.  854437.  511900.  735000.  319500.  799900.  651948.  490000.  599900.  229000.\n",
            "  522075.  509990. 1250000.  650000.  589900.  720000.  775000.  445000.  220000.  347000.  614900. 1005685.  360000.  699990.  324000.  477771.  750000.  465000.  785000.\n",
            "  930525.  450000.  189500.  655000.  571900.  364900.  575000.  475990. 1575000.  523900.  635000.  499990.  464500.  540000.  464900.  478900.]\n",
            "\n",
            "The weights are\t [634761.25790238 -34494.86485706 -81610.58952415  10254.95401817  15948.93520383 -16568.09362864 -10428.47503448 -36557.31328325 202954.09477335  29300.70217024 105893.24004436\n",
            "   2361.24955799 -18553.07931308  14100.13475879 100016.74287451  23103.78018092 -30947.09512776]\n",
            "\n",
            "The mean absolute error on the test set is\t 88908.15572230352 \n",
            "\n",
            "------------\n",
            "Using the lrSchedual gradient variant\n",
            "-------------\n",
            "Epoch  0\n",
            "Initial starting points:  [0.44199776 0.35667675 0.4447479  0.34188905 0.12671276 0.7031373  0.84192755 0.43810928 0.35955676 0.04049022 0.11381913 0.69844245 0.50531091 0.9076776  0.47448985 0.16126703\n",
            " 0.38532298]\n",
            "Gradient descent failed\n",
            "Initial starting points:  [0.98308743 0.21622134 0.96648123 0.30649332 0.13954218 0.71036322 0.41525537 0.15991879 0.62570517 0.59269188 0.77593965 0.0036728  0.64994984 0.91198422 0.39970756 0.87673872\n",
            " 0.61010897]\n",
            "Gradient descent failed\n",
            "Epoch  1\n",
            "Initial starting points:  [0.01229971 0.50865428 0.88890783 0.32269496 0.64690025 0.82887812 0.50660798 0.08245303 0.50264732 0.26655867 0.14985885 0.97468366 0.6563627  0.70499642 0.52623225 0.18845802\n",
            " 0.94870371]\n",
            "Gradient descent failed\n",
            "Initial starting points:  [0.55683217 0.83126372 0.71413631 0.4025885  0.62768768 0.0477856  0.33610411 0.0132933  0.76288735 0.13364018 0.43253505 0.27249334 0.53193091 0.45629105 0.79110306 0.69914103\n",
            " 0.7790472 ]\n",
            "Gradient descent failed\n",
            "Finding the best parameters at the epoch of  0\n",
            "\n",
            "Fitting time is 16.767405033111572\n",
            "\n",
            "Predicting the model...\n",
            "\n",
            "Prediction time is 9.083747863769531e-05\n",
            "\n",
            "The r^2 score is\t 0.9382950520219118\n",
            "\n",
            "The mean absolute error on the training set is\t 56305.727405389116\n",
            "\n",
            "The predicted y values for the test set are\t [ 494426.  726599.  194054.  660816.  614813.  502575. 1008868.  226799.  586574.  323530.  729905.  503266.  350834.  112098.  677598. 1381571. 1049112.  549131.  604425.\n",
            "  675969.  670583.  697544. 2013777.  566526.  773956. 1131719.  637799.  795734.  504825.  487537. 1247219.  436431.  550857.  773943.  524459.  271180.  442128.  706538.\n",
            " 1332974. 1012579.  649839.  278532. 1285201.  514297.  953049.  709082.  597628.  684080.  530024.  509711.  659097.  455158.  557709.  566622.  915671.  595047.  224909.\n",
            "  529318.  452727.  307972.  293040.  367541.  473323.  333771.  806697.  297080.  694482. 1394239.  453489.  492269.  135957. 1170284.  777386.  622023.  239845.  398314.\n",
            "  449971.  482928. 1162346. 1190989. 1173324. 1224957.  773417.  724045.  594633.  491210.  631106.  553471.  671598.  614814.  695349. 1372717.  276833.  381843.  461402.\n",
            "  454062.  365645.  800408.  916158.  939110.  714326. 1665832.  736605.  720753.  687184.  700942.  400234.  285561.  593599.  720712.  333643.  670916.  802636.  725724.\n",
            "  337850.  715171.  922488.  502259.  453612.  634687.  173067.  471849.  511818.  556377. 1557089.  707246.  600749.  310494.  381507.  657649.  574754. 1256464.  586906.\n",
            "  174735.  483501.  234605.  551547.  856864.  311098.  655498.  683441.  537205.  618366.  549505.  608391.  578913.  314947.  930801.  710887.  476759.  647750.  310473.\n",
            "  575762.  466005. 1053375.  754709.  582837.  712407.  731955.  425297.  211258.  334866.  635816. 1378116.  283457.  748924.  351066.  505643.  685595.  456594.  846733.\n",
            "  571802.  505643.  194054.  573950.  575591.  275888.  713792.  450751. 1390340.  597226.  763411.  533076.  437059.  552105.  491604.  548640.]\n",
            "The real y values for the test set are\t [ 525000.  575000.  179900.  599000.  795000.  503990.  875000.  232900.  469900.  349900.  725900.  600000.  425000.  226900.  649900. 1112885.  999999.  505000.  756500.\n",
            "  879000.  569900.  512000. 1699000.  561900.  689950. 1199000.  569900.  715000.  426990.  416990. 1375000.  497900.  529900.  875000.  509000.  237000.  440000.  840000.\n",
            "  827655. 1399000.  699900.  245000. 1345000.  505900.  850000.  750000.  689000.  589900.  639000.  421999.  650000.  445000.  456990.  630000.  975000.  564775.  341500.\n",
            "  505990.  406990.  359500.  349999.  369000.  508990.  450000.  739000.  329900.  735000. 1500000.  474990.  500000.  124900.  899000.  665000.  549900.  327000.  429000.\n",
            "  508000.  446900.  925000. 1050000. 1450000. 1199000.  795000.  750000.  659900.  479900.  564900.  478900.  574900.  534900.  613290. 1800000.  362500.  469900.  439990.\n",
            "  472900.  445000.  832140.  868000.  698980.  609900. 1818000.  874900.  669900.  935000.  530000.  325000.  339000.  511000.  875000.  369900.  769900.  698000.  729900.\n",
            "  389900.  774650. 1136900.  486990.  529000.  617500.  205900.  437990.  430000.  574900.  891846.  875000.  522000.  394000.  425000.  765135.  530000. 1595000.  534000.\n",
            "  119900.  436625.  224900.  564900.  840000.  369900.  827189.  840000.  763634.  615000.  854437.  511900.  735000.  319500.  799900.  651948.  490000.  599900.  229000.\n",
            "  522075.  509990. 1250000.  650000.  589900.  720000.  775000.  445000.  220000.  347000.  614900. 1005685.  360000.  699990.  324000.  477771.  750000.  465000.  785000.\n",
            "  930525.  450000.  189500.  655000.  571900.  364900.  575000.  475990. 1575000.  523900.  635000.  499990.  464500.  540000.  464900.  478900.]\n",
            "\n",
            "The weights are\t [634761.25817288 -34494.86410093 -81610.58936594  10255.36850504  15948.53968866 -16568.21767633 -10428.47488135 -36557.31322574 202954.09616415  29300.70224999 105893.24041719\n",
            "   2361.25019379 -18553.07954235  14100.13439633 100016.74272514  23103.78085249 -30947.09509123]\n",
            "\n",
            "The mean absolute error on the test set is\t 88908.15527865576 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2 Logistic Classification"
      ],
      "metadata": {
        "id": "cfam-mjDeHJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pre_6. Re-Buiding BinaryLogisticClassifier() model for breast_cancer dataset combination with explicit gradient method"
      ],
      "metadata": {
        "id": "aERVwGhWsQys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BinaryLogisticClassifier():\n",
        "    # fit the model to the data\n",
        "    def fit(self,  X, y, w0, alpha, h, tolerance, max_iterations, explicit = False ):\n",
        "\n",
        "        X = np.hstack((np.ones([X.shape[0], 1]), X))\n",
        "\n",
        "        # find the w values that minimize the sum of squared errors via gradient descent\n",
        "        L = lambda w: ((self.sigmoid(X @ w)).T - y.T) @ (self.sigmoid(X @ w) - y)\n",
        "\n",
        "        self.w = self.gradientDescent(X, L, y, w0, alpha, h, tolerance, max_iterations, use_explicit_gradient = explicit)\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1/(1+np.exp(-z))\n",
        "    # predict the output from testing data\n",
        "    def predict(self, X):\n",
        "        # adding a column in to X matrix as intercept\n",
        "        X = np.hstack((np.ones([X.shape[0],1]), X))\n",
        "        return self.sigmoid(X @ self.w)\n",
        "\n",
        "    # run gradient descent and output the coordinates of the estimated critical point\n",
        "    def gradientDescent(self, X, f, y, x0, alpha, h, tolerance, maxIterations, use_explicit_gradient = False):\n",
        "        # set x equal to the initial guess\n",
        "        x = x0\n",
        "\n",
        "        # take up to maxIterations number of steps\n",
        "        for counter in range(maxIterations):\n",
        "            if use_explicit_gradient:\n",
        "              gradient = self.compute_explicit_gradient(X, y, x0)\n",
        "            else:\n",
        "              gradient = self.computeGradient(f, x, h)\n",
        "\n",
        "            # stop if the norm of the gradient is near 0 (success)\n",
        "            if np.linalg.norm(gradient) < tolerance:\n",
        "                print('Gradient descent took', counter, 'iterations to converge')\n",
        "                # print('The norm of the gradient is', np.linalg.norm(gradient))\n",
        "\n",
        "                # return the approximate critical point x\n",
        "                return x\n",
        "\n",
        "            # print a message if we do not converge (failure)\n",
        "            elif counter == maxIterations-1:\n",
        "                print(\"Gradient descent failed\")\n",
        "                # print('The gradient is', gradient)\n",
        "\n",
        "                # return x, sometimes it is still pretty good\n",
        "                return x\n",
        "\n",
        "            # take a step in the opposite direction as the gradient\n",
        "            x -= alpha*gradient\n",
        "\n",
        "    # estimate the gradient\n",
        "    def computeGradient(self, f, x, h):\n",
        "        n = len(x)\n",
        "        gradient = np.zeros(n)\n",
        "\n",
        "        for counter in range(n):\n",
        "            xUp = x.copy()\n",
        "            xUp[counter] += h\n",
        "            gradient[counter] = (f(xUp) - f(x))/h\n",
        "\n",
        "        return gradient\n",
        "\n",
        "    # Explicit gradient\n",
        "    def compute_explicit_gradient(self, X, y, w):\n",
        "\n",
        "        gradient = 2 *np.dot(X.T, (self.sigmoid(np.dot(X, w)) - y))\n",
        "\n",
        "        return gradient\n"
      ],
      "metadata": {
        "id": "UHspHCmxC2RY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "# read the breast cancer dataset\n",
        "breast_cancer_data = load_breast_cancer()\n",
        "\n",
        "# find the data and labels\n",
        "X = breast_cancer_data.data\n",
        "Y = np.array(breast_cancer_data.target, dtype= float)\n",
        "\n",
        "\n",
        "# split the data into training and test sets\n",
        "trainX, testX, trainY, testY = train_test_split(X, Y, train_size = 0.75, random_state =42)\n",
        "\n",
        "trainX = (trainX - np.mean(trainX, axis=0))/np.std(trainX, axis = 0)\n",
        "testX = (testX - np.mean(testX, axis=0))/np.std(testX, axis = 0)\n"
      ],
      "metadata": {
        "id": "ozQETRKHHPz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test model using numerical gradient"
      ],
      "metadata": {
        "id": "XxtWXR-U3B5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BinaryLogisticClassifier()\n",
        "\n",
        "model.fit(trainX, trainY, (trainX.shape[1] +1)*[0], 0.001, 0.001, 0.001, 10000, explicit = False)\n",
        "predictedY = model.predict(trainX)\n",
        "\n",
        "plt.plot(predictedY, '.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "Zs0_th1r4csm",
        "outputId": "ddb5aa39-cba5-4f94-cd07-a027d78c3259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient descent failed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f278ae25c30>]"
            ]
          },
          "metadata": {},
          "execution_count": 239
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test model using explicit gradient"
      ],
      "metadata": {
        "id": "AesE9WSN3GOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BinaryLogisticClassifier()\n",
        "\n",
        "model.fit(trainX, trainY, (trainX.shape[1] +1)*[0], 0.001, 0.001, 0.001, 10000, explicit = True)\n",
        "predictedY = model.predict(trainX)\n",
        "\n",
        "plt.plot(predictedY, '.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "Mok42wna5DrK",
        "outputId": "79c5e2e0-cedd-4576-85df-8c9bfd2b699b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient descent failed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-237-5ff6ba6d33c5>:13: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-z))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f278ac34c10>]"
            ]
          },
          "metadata": {},
          "execution_count": 240
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6 & 7. Mutil classes classifier Using \"Cross-entropy loss\" with two options\n",
        "(1) numerical gradient\n",
        "\n",
        "(2) explicit gradient (exact gradient from the loss function)\n",
        "\n",
        "###Notes: In this problem, I use Cross-entropy loss istead of MSE loss\n"
      ],
      "metadata": {
        "id": "PuJbqQ94SYGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class v3MultiLogisticClassifier():\n",
        "    def fit(self, X, y, w0, alpha, h, tolerance, max_iterations, explicit=False):\n",
        "        X = np.hstack((np.ones([X.shape[0], 1]), X))\n",
        "        num_samples = len(y)\n",
        "\n",
        "        epsilon = 1e-15  # Small constant to avoid division by zero or taking log of zero\n",
        "\n",
        "        # cross-entropy loss\n",
        "        L = lambda w: -np.sum(np.multiply(y, np.log(np.clip(self.softmax(X @ w), epsilon, 1 - epsilon)))) / num_samples\n",
        "\n",
        "        self.w = self.gradientDescent(X, L, y, w0, alpha, h, tolerance, max_iterations, use_explicit_gradient=explicit)\n",
        "\n",
        "        return self.w\n",
        "\n",
        "    def softmax(self, z):\n",
        "        e_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "        return e_z / e_z.sum(axis=1, keepdims=True)\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.hstack((np.ones([X.shape[0], 1]), X))\n",
        "        scores = X @ self.w\n",
        "        probabilities = self.softmax(scores)\n",
        "        return  np.argmax(probabilities, axis=1)\n",
        "\n",
        "    def gradientDescent(self, X, f, y, w0, alpha, h, tolerance, maxIterations, use_explicit_gradient=False):\n",
        "        # Initialize x (weights) with the same shape as w0\n",
        "        x = w0.copy()\n",
        "\n",
        "        # take up to maxIterations number of steps\n",
        "        for counter in range(maxIterations):\n",
        "            if use_explicit_gradient:\n",
        "              gradient = self.compute_explicit_gradient(X, y, x)\n",
        "            else:\n",
        "              gradient = self.computeGradient(f, x, h)\n",
        "\n",
        "            # stop if the norm of the gradient is near 0 (success)\n",
        "            if np.linalg.norm(gradient) < tolerance:\n",
        "                print('Gradient descent took', counter, 'iterations to converge')\n",
        "\n",
        "                # return the approximate critical point x\n",
        "                return x\n",
        "\n",
        "            # print a message if we do not converge (failure)\n",
        "            elif counter == maxIterations-1:\n",
        "                print(\"Gradient descent failed\")\n",
        "                # print('The gradient is', gradient)\n",
        "\n",
        "                return x\n",
        "            # print(X.shape, x.shape, gradient.shape)\n",
        "            # take a step in the opposite direction as the gradient\n",
        "            x -= alpha*gradient\n",
        "\n",
        "    # estimate the gradient\n",
        "    def computeGradient(self, f, x, h):\n",
        "        n =  x.shape[0]\n",
        "        gradient = np.zeros((x.shape[0], x.shape[1]))\n",
        "        fx = f(x)\n",
        "\n",
        "        for count in range(n):\n",
        "            xUp = x.copy()\n",
        "            xUp[count] += h\n",
        "            gradient[count] = (f(xUp) - fx)/h\n",
        "        return gradient\n",
        "\n",
        "    # Explicit gradient\n",
        "    def compute_explicit_gradient(self, X, y, w):\n",
        "        N = len(y)\n",
        "        gradient = (1/N) *np.dot(X.T, (self.softmax(X@ w) - y))\n",
        "        # print(gradient.shape)\n",
        "        return gradient\n",
        "\n"
      ],
      "metadata": {
        "id": "0SBKUOB6STxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test on MNIST dataset"
      ],
      "metadata": {
        "id": "kagdpsSz6qYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Load the MNIST dataset from scikit-learn\n",
        "# mnist = fetch_openml(\"mnist_784\")\n",
        "mnist = datasets.load_digits()\n",
        "X = mnist.data.astype(float)\n",
        "y = mnist.target.astype(int)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize your MultiLogisticClassifier\n",
        "clf = v3MultiLogisticClassifier()\n",
        "\n",
        "# Set hyperparameters\n",
        "alpha = 0.001\n",
        "h = 0.01\n",
        "tolerance = 1e-4\n",
        "max_iterations = 10000\n",
        "explicit = True\n",
        "\n",
        "num_features = X_train.shape[1] + 1  # Plus one for the bias term\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "w0 = np.random.rand(num_features, num_classes)*0.001\n",
        "\n",
        "# Convert class labels to one-hot encoded labels\n",
        "encoder = LabelBinarizer()\n",
        "y_train_encoded = encoder.fit_transform(y_train)\n",
        "y_test_encoded = encoder.transform(y_test)\n",
        "\n",
        "\n",
        "# Fit the model on the training data\n",
        "clf.fit(X_train, y_train_encoded, w0, alpha, h, tolerance, max_iterations, explicit)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean(y_pred == y_test)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0RCPTbCTbAQ",
        "outputId": "fe364eed-33bc-4ce6-efcb-42c6fffb9f39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient descent failed\n",
            "Accuracy: 0.975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Plot the result"
      ],
      "metadata": {
        "id": "xshXeCFxck9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, axes = plt.subplots(nrows=1, ncols=5, figsize=(10, 3))\n",
        "for ax, image, prediction in zip(axes, X_test, y_pred):\n",
        "    ax.set_axis_off()\n",
        "    image = image.reshape(8, 8)\n",
        "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
        "    ax.set_title(f\"Prediction: {prediction}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "iXZTmjEuLnj4",
        "outputId": "fdffb9b9-c6b6-483f-fcb3-8319d8bc22fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x300 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8.  Add functionality to optimize models using the three variants of gradient descent from Problems 2-4."
      ],
      "metadata": {
        "id": "8CPccA4B562E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding functionalities\n",
        "\n",
        "(1) Single gradient\n",
        "\n",
        "(2) Using mutil-initial points gradient\n",
        "\n",
        "(3) Using cyclic learning rate gradient"
      ],
      "metadata": {
        "id": "bVKGFHO-vwDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier():\n",
        "\n",
        "    def fit(self, num_epochs, num_points, X, y, w0, alpha, h, tolerance, max_iterations, gradient_type: str, explicit = False):\n",
        "\n",
        "        X = np.hstack((np.ones([X.shape[0], 1]), X))\n",
        "\n",
        "        num_samples = len(y)\n",
        "        epsilon = 1e-15  # Small constant to avoid division by zero or taking log of zero\n",
        "\n",
        "        L = lambda w: -np.sum(np.multiply(y, np.log(np.clip(self.softmax(X @ w), epsilon, 1 - epsilon)))) / num_samples\n",
        "\n",
        "        # 8. Add functionality to optimize models ['simple' 'multipoint' 'lrSchedual']\n",
        "        if gradient_type == 'simple':\n",
        "          print(f'------------\\nUsing the {gradient_type} gradient variant\\n-------------')\n",
        "          self.w = self.gradientDescent( X, L, y, w0, alpha, h, tolerance, max_iterations, explicit)\n",
        "          self.all = None\n",
        "\n",
        "        elif gradient_type == 'multipoint':\n",
        "          print(f'------------\\nUsing the {gradient_type} gradient variant\\n-------------')\n",
        "          self.w, _ , _ = self.run_optimization(num_points, X, L, y, alpha, h, tolerance, max_iterations, explicit = explicit)\n",
        "\n",
        "        elif gradient_type == 'lrSchedual':\n",
        "          print(f'------------\\nUsing the {gradient_type} gradient variant\\n-------------')\n",
        "          self.w, _, self.all = self.gradient_descent_lrSchedual(num_epochs, num_points, X, L, y, alpha, h, tolerance, max_iterations, explicit)\n",
        "\n",
        "        return self.w\n",
        "\n",
        "    def softmax(self, z):\n",
        "        e_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "        return e_z / e_z.sum(axis=1, keepdims=True)\n",
        "\n",
        "    # predict the output from testing data\n",
        "    def predict(self, X):\n",
        "        # adding a column in to X matrix as intercept\n",
        "        X = np.hstack((np.ones([X.shape[0],1]), X))\n",
        "        # if self.all is None:\n",
        "        scores = X @ self.w\n",
        "        probabilities = self.softmax(scores)\n",
        "\n",
        "        return np.argmax(probabilities, axis=1)\n",
        "\n",
        "    # 2. Gradient Descent with n random points\n",
        "    def run_optimization(self, num_points, X, f, y, alpha, h, tolerance, maxIterations, explicit = True):\n",
        "        best_params = None\n",
        "        best_loss = float('inf')\n",
        "        all_weight = []\n",
        "\n",
        "        for _ in range(num_points):\n",
        "            # Generate a random starting point\n",
        "            initial_theta = np.random.rand(X.shape[1], y.shape[1])\n",
        "            print('Initial starting points: ', initial_theta.shape )\n",
        "\n",
        "            optimized_theta = self.gradientDescent( X, f, y, initial_theta, alpha, h, tolerance, max_iterations, explicit)\n",
        "\n",
        "            # Calculate the loss for the optimized parameters\n",
        "            current_loss = f(optimized_theta)\n",
        "\n",
        "            # Check if this run has the lowest loss so far\n",
        "            if current_loss < best_loss:\n",
        "                best_loss = current_loss\n",
        "                best_params = optimized_theta\n",
        "\n",
        "            all_weight.append(optimized_theta)\n",
        "\n",
        "        return best_params, best_loss, all_weight\n",
        "\n",
        "    # gradient with epoch\n",
        "    def gradient_descent_lrSchedual(self, num_epochs, num_points, X, f, y, alpha, h, tolerance, maxIterations, explicit):\n",
        "        # Gradient Descent optimization algorithm with cyclic learning rate.\n",
        "        best_params = None\n",
        "        best_loss = float('inf')\n",
        "        all_best_w = []\n",
        "        all_best_loss = []\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            learning_rate = alpha / (epoch + 1) # reduce learning rate for every epoch\n",
        "            print('Epoch ', epoch)\n",
        "            x, loss , all_weights = self.run_optimization(num_points, X, f, y, alpha, h, tolerance, maxIterations, explicit = explicit)\n",
        "\n",
        "            # Save parameters every time the learning rate vanishes\n",
        "            np.savetxt(f\"model_parameters_epoch_{epoch}.txt\", x)\n",
        "\n",
        "            # Calculate the loss for the optimized parameters\n",
        "            current_loss = loss\n",
        "\n",
        "            # Check if this run has the lowest loss so far\n",
        "            if current_loss < best_loss:\n",
        "                best_loss = current_loss\n",
        "                best_params = x\n",
        "                best_epoch = epoch\n",
        "        all_best_w.append(best_params)\n",
        "        all_best_loss.append(best_loss)\n",
        "        print('Finding the best parameters at the epoch of ', best_epoch)\n",
        "\n",
        "        return best_params, all_best_loss, all_best_w\n",
        "\n",
        "    def gradientDescent(self, X, f, y, w0, alpha, h, tolerance, maxIterations, use_explicit_gradient= True):\n",
        "        # Initialize x (weights) with the same shape as w0\n",
        "        x = w0.copy()\n",
        "\n",
        "        # take up to maxIterations number of steps\n",
        "        for counter in range(maxIterations):\n",
        "            if use_explicit_gradient:\n",
        "              gradient = self.compute_explicit_gradient(X, y, x)\n",
        "            else:\n",
        "              gradient = self.computeGradient(f, x, h)\n",
        "\n",
        "            # stop if the norm of the gradient is near 0 (success)\n",
        "            if np.linalg.norm(gradient) < tolerance:\n",
        "                print('Gradient descent took', counter, 'iterations to converge')\n",
        "\n",
        "                return x\n",
        "\n",
        "            # print a message if we do not converge (failure)\n",
        "            elif counter == maxIterations-1:\n",
        "                print(\"Gradient descent failed\")\n",
        "\n",
        "                return x\n",
        "            # take a step in the opposite direction as the gradient\n",
        "            x -= alpha*gradient\n",
        "\n",
        "    # estimate the gradient\n",
        "    def computeGradient(self, f, x, h):\n",
        "        n =  x.shape[0]\n",
        "        gradient = np.zeros((x.shape[0], x.shape[1]))\n",
        "        fx = f(x)\n",
        "\n",
        "        for count in range(n):\n",
        "            xUp = x.copy()\n",
        "            xUp[count] += h\n",
        "            gradient[count] = (f(xUp) - fx)/h\n",
        "        return gradient\n",
        "\n",
        "    # Explicit gradient\n",
        "    def compute_explicit_gradient(self, X, y, w):\n",
        "        N = len(y)\n",
        "        # print(self.softmax(X@ w).shape, len(y))\n",
        "\n",
        "        gradient = (1/N) *np.dot(X.T, (self.softmax(np.dot(X, w)) - y))\n",
        "\n",
        "        return gradient\n"
      ],
      "metadata": {
        "id": "-dHwHJ0ScxTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Test the added functions on the MNIST dataset"
      ],
      "metadata": {
        "id": "0-yaUco1vmqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Load the MNIST dataset from scikit-learn\n",
        "mnist = datasets.load_digits()\n",
        "X = mnist.data.astype(float)\n",
        "y = mnist.target.astype(int)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Set hyperparameters\n",
        "alpha = 0.001\n",
        "h = 0.01\n",
        "tolerance = 1e-4\n",
        "max_iterations = 10000\n",
        "explicits = [True]\n",
        "\n",
        "num_features = X_train.shape[1] + 1  # Plus one for the bias term\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "w0 = np.random.rand(num_features, num_classes)*0.001\n",
        "\n",
        "# Convert class labels to one-hot encoded labels\n",
        "encoder = LabelBinarizer()\n",
        "y_train_encoded = encoder.fit_transform(y_train)\n",
        "y_test_encoded = encoder.transform(y_test)\n",
        "\n",
        "num_epochs, num_points = 2, 1\n",
        "\n",
        "gradient_type = ['lrSchedual'] #  'simple' 'multipoint' 'lrSchedual'\n",
        "\n",
        "\n",
        "for explicit in explicits:\n",
        "  if explicit: print('\\nUsing explicit gradient')\n",
        "  else: print('\\nUsing numerical gradient method')\n",
        "  for g_type in gradient_type:\n",
        "    model = Classifier()\n",
        "    model.fit(num_epochs, num_points,X_train, y_train_encoded, w0, alpha, h, tolerance, max_iterations, gradient_type = g_type, explicit = explicit)\n",
        "\n",
        "    prediction = model.predict(X_test)\n",
        "    accuracy = np.mean(prediction == y_test)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "\n",
        "    print('Mean of predictions: ', prediction)\n",
        "    print('\\nReal values: ', y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QVg4EbQc8KM",
        "outputId": "f249b5a2-2fef-4e20-b8e6-1e78a39ad39c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using explicit gradient\n",
            "------------\n",
            "Using the lrSchedual gradient variant\n",
            "-------------\n",
            "Epoch  0\n",
            "Initial starting points:  (65, 10)\n",
            "Gradient descent failed\n",
            "Epoch  1\n",
            "Initial starting points:  (65, 10)\n",
            "Gradient descent failed\n",
            "Finding the best parameters at the epoch of  0\n",
            "Accuracy: 0.9444444444444444\n",
            "Mean of predictions:  [6 9 3 7 2 1 5 2 5 2 3 4 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 9 5 6 0 9 0 0 1 5 4 5 2 4 5 7 0 7 5 9 5 5 4 7 0 1 5 5 9 9 0 2 3 8 0 6 4 4\n",
            " 9 1 2 8 3 5 2 8 0 4 4 1 3 5 3 1 3 5 9 4 2 7 7 4 4 1 9 2 7 8 7 2 6 9 4 0 7 2 7 5 6 7 5 7 7 0 6 6 4 2 8 0 9 4 6 9 9 6 9 0 3 5 6 6 0 6 4 3 9 3 9 7 2 9 0 4 5 3 6 5 9 9 8 4 2 1 3 7 7\n",
            " 2 2 3 9 8 0 3 2 3 5 6 9 9 4 1 0 4 2 3 6 4 8 5 9 5 7 1 9 4 8 1 5 4 4 9 6 1 8 6 0 4 5 2 7 1 6 4 5 6 0 3 2 3 6 7 1 5 1 4 7 6 5 8 5 4 1 5 2 8 8 4 5 7 6 2 2 2 3 4 8 9 3 6 0 9 7 7 0 1\n",
            " 0 4 5 1 5 3 6 0 4 1 0 0 3 6 5 9 7 3 5 5 9 9 8 5 3 3 2 0 5 8 3 4 0 2 4 6 4 3 4 5 0 5 2 1 3 1 4 1 1 7 0 1 5 2 1 2 8 7 0 6 4 8 1 5 1 8 4 5 8 7 9 8 6 0 6 2 0 7 9 8 9 5 2 7 7 1 8 7 4\n",
            " 3 8 3 5 6 0 0 3 0 5 0 0 4 1 8 8 4 5 9 6 3 4 8 8 4 2 3 6 9 8 8 5 0 6 3 3 7 1 6 4 1 2 1 1 5 4 7 4 8 3 4 0 5 1 9 4 5 7 6 3 7 0 5 9 7 5 9 7 4 2 1 9 0 7 5 2 3 6 3 9 6 9 5 0 1 5 5 8 7\n",
            " 3 6 2 6 5]\n",
            "\n",
            "Real values:  [6 9 3 7 2 1 5 2 5 2 1 9 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 9 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 5 5 4 7 0 4 5 5 9 9 0 2 3 8 0 6 4 4\n",
            " 9 1 2 8 3 5 2 9 0 4 4 4 3 5 3 1 3 5 9 4 2 7 7 4 4 1 9 2 7 8 7 2 6 9 4 0 7 2 7 5 8 7 5 7 7 0 6 6 4 2 8 0 9 4 6 9 9 6 9 0 3 5 6 6 0 6 4 3 9 3 9 7 2 9 0 4 5 3 6 5 9 9 8 4 2 1 3 7 7\n",
            " 2 2 3 9 8 0 3 2 2 5 6 9 9 4 1 5 4 2 3 6 4 8 5 9 5 7 8 9 4 8 1 5 4 4 9 6 1 8 6 0 4 5 2 7 4 6 4 5 6 0 3 2 3 6 7 1 5 1 4 7 6 8 8 5 5 1 6 2 8 8 9 9 7 6 2 2 2 3 4 8 8 3 6 0 9 7 7 0 1\n",
            " 0 4 5 1 5 3 6 0 4 1 0 0 3 6 5 9 7 3 5 5 9 9 8 5 3 3 2 0 5 8 3 4 0 2 4 6 4 3 4 5 0 5 2 1 3 1 4 1 1 7 0 1 5 2 1 2 8 7 0 6 4 8 8 5 1 8 4 5 8 7 9 8 5 0 6 2 0 7 9 8 9 5 2 7 7 1 8 7 4\n",
            " 3 8 3 5 6 0 0 3 0 5 0 0 4 1 2 8 4 5 9 6 3 1 8 8 4 2 3 8 9 8 8 5 0 6 3 3 7 1 6 4 1 2 1 1 6 4 7 4 8 3 4 0 5 1 9 4 5 7 6 3 7 0 5 9 7 5 9 7 4 2 1 9 0 7 5 3 3 6 3 9 6 9 5 0 1 5 5 8 3\n",
            " 3 6 2 6 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##9. Apply your classifier and all six variants of gradient descent on the MNIST dataset from sklearn"
      ],
      "metadata": {
        "id": "LUzTBeLK6CgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Load the MNIST dataset from scikit-learn\n",
        "mnist = datasets.load_digits()\n",
        "X = mnist.data.astype(float)\n",
        "y = mnist.target.astype(int)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Convert class labels to one-hot encoded labels\n",
        "encoder = LabelBinarizer()\n",
        "y_train_encoded = encoder.fit_transform(y_train)\n",
        "y_test_encoded = encoder.transform(y_test)\n",
        "\n",
        "# Set hyperparameters\n",
        "alpha = 0.001\n",
        "h = 0.01\n",
        "tolerance = 1e-4\n",
        "max_iterations = 10000\n",
        "\n",
        "num_features = X_train.shape[1] + 1  # Plus one for the bias term\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "w0 = np.random.rand(num_features, num_classes)*0.001\n",
        "\n",
        "explicits = [True, False]\n",
        "gradient_type = ['simple', 'multipoint', 'lrSchedual']\n",
        "num_epochs = 2\n",
        "num_points = 2 # sets initial values\n",
        "\n",
        "for explicit in explicits:\n",
        "  if explicit: print('\\nUsing explicit gradient\\n-------------------------------------------------------------')\n",
        "  else: print('\\nUsing numerical gradient method\\n-------------------------------------------------------------')\n",
        "\n",
        "  for g_type in gradient_type:\n",
        "    model = Classifier()\n",
        "    # fitting model\n",
        "    fstart = time.time()\n",
        "    model.fit(num_epochs, num_points,X_train, y_train_encoded, w0, alpha, h, tolerance, max_iterations, gradient_type = g_type, explicit = explicit)\n",
        "    fend = time.time()\n",
        "    print(f\"\\nFitting time is {fend - fstart}\")\n",
        "    print('\\nPredicting the model...\\n')\n",
        "\n",
        "    trainPredictions_num_all = model.predict(X)\n",
        "\n",
        "    pstart = time.time()\n",
        "    trainPredictions_num = model.predict(X_train)\n",
        "    pend = time.time()\n",
        "    print(f\"Prediction time is {pend - pstart}\")\n",
        "\n",
        "    prediction = model.predict(X_test)\n",
        "    accuracy = np.mean(prediction == y_test)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "\n",
        "    print('Mean of predictions: ', prediction)\n",
        "    print('\\nReal values: ', y_test)\n",
        "\n",
        "    print('\\nThe r^2 score is\\t', r2_score(y_train, trainPredictions_num))\n",
        "\n",
        "\n",
        "    # print quality metrics\n",
        "    print('\\nThe mean absolute error on the training set is\\t', mean_absolute_error(y_train, trainPredictions_num))\n",
        "\n",
        "    # return the predicted outputs for the datapoints in the test set\n",
        "    predictions_num = model.predict(X_test)\n",
        "\n",
        "    # print the predictions\n",
        "    print('\\nThe predicted y values for the test set are\\t', np.round(predictions_num,0))\n",
        "\n",
        "    # print the real y values\n",
        "    print('The real y values for the test set are\\t', y_test)\n",
        "\n",
        "    # print the weights\n",
        "    # print('\\nThe weights are\\t', model.w)\n",
        "\n",
        "    # print quality metrics\n",
        "    print('\\nThe mean absolute error on the test set is\\t', mean_absolute_error(y_test, predictions_num), '\\n')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWKBFf5IwxYm",
        "outputId": "cfdc6bce-0ac9-4adc-cb8d-64654fd5edb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using explicit gradient\n",
            "-------------------------------------------------------------\n",
            "------------\n",
            "Using the simple gradient variant\n",
            "-------------\n",
            "Gradient descent failed\n",
            "\n",
            "Fitting time is 5.3680994510650635\n",
            "\n",
            "Predicting the model...\n",
            "\n",
            "Prediction time is 0.0007991790771484375\n",
            "Accuracy: 0.9733333333333334\n",
            "Mean of predictions:  [6 9 3 7 2 2 5 2 5 2 1 9 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 9 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 5 5 4 7 0 4 5 5 9 9 0 2 3 8 0 6 4 4\n",
            " 9 1 2 8 3 5 2 9 0 4 4 4 3 5 3 1 3 5 9 4 2 7 7 4 4 1 9 2 7 8 7 2 6 9 4 0 7 2 7 5 8 7 5 7 9 0 6 6 4 2 8 0 9 4 6 9 9 6 9 0 5 5 6 6 0 6 4 3 9 3 8 7 2 9 0 4 5 8 6 5 9 9 8 4 2 1 3 7 7\n",
            " 2 2 3 9 8 0 3 2 2 5 6 9 9 4 1 2 4 2 3 6 4 8 5 9 5 7 8 9 4 8 1 5 4 4 9 6 1 8 6 0 4 5 2 7 1 6 4 5 6 0 3 2 3 6 7 1 9 1 4 7 6 5 8 5 5 1 5 2 8 8 9 9 7 6 2 2 2 3 4 8 8 3 6 0 9 7 7 0 1\n",
            " 0 4 5 1 5 3 6 0 4 1 0 0 3 6 5 9 7 3 5 5 9 9 8 5 3 3 2 0 5 8 3 4 0 2 4 6 4 3 4 5 0 5 2 1 3 1 4 1 1 7 0 1 5 2 1 2 8 7 0 6 4 8 8 5 1 8 4 5 8 7 9 8 6 0 6 2 0 7 9 8 9 5 2 7 7 1 8 7 4\n",
            " 3 8 3 5 6 0 0 3 0 5 0 0 4 1 2 8 4 5 9 6 3 1 8 8 4 2 3 8 9 8 8 5 0 6 3 3 7 1 6 4 1 2 1 1 6 4 7 4 8 3 4 0 5 1 9 4 5 7 6 3 7 0 5 9 7 5 9 7 4 2 1 9 0 7 5 8 3 6 3 9 6 9 5 0 1 5 5 8 3\n",
            " 3 6 2 6 5]\n",
            "\n",
            "Real values:  [6 9 3 7 2 1 5 2 5 2 1 9 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 9 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 5 5 4 7 0 4 5 5 9 9 0 2 3 8 0 6 4 4\n",
            " 9 1 2 8 3 5 2 9 0 4 4 4 3 5 3 1 3 5 9 4 2 7 7 4 4 1 9 2 7 8 7 2 6 9 4 0 7 2 7 5 8 7 5 7 7 0 6 6 4 2 8 0 9 4 6 9 9 6 9 0 3 5 6 6 0 6 4 3 9 3 9 7 2 9 0 4 5 3 6 5 9 9 8 4 2 1 3 7 7\n",
            " 2 2 3 9 8 0 3 2 2 5 6 9 9 4 1 5 4 2 3 6 4 8 5 9 5 7 8 9 4 8 1 5 4 4 9 6 1 8 6 0 4 5 2 7 4 6 4 5 6 0 3 2 3 6 7 1 5 1 4 7 6 8 8 5 5 1 6 2 8 8 9 9 7 6 2 2 2 3 4 8 8 3 6 0 9 7 7 0 1\n",
            " 0 4 5 1 5 3 6 0 4 1 0 0 3 6 5 9 7 3 5 5 9 9 8 5 3 3 2 0 5 8 3 4 0 2 4 6 4 3 4 5 0 5 2 1 3 1 4 1 1 7 0 1 5 2 1 2 8 7 0 6 4 8 8 5 1 8 4 5 8 7 9 8 5 0 6 2 0 7 9 8 9 5 2 7 7 1 8 7 4\n",
            " 3 8 3 5 6 0 0 3 0 5 0 0 4 1 2 8 4 5 9 6 3 1 8 8 4 2 3 8 9 8 8 5 0 6 3 3 7 1 6 4 1 2 1 1 6 4 7 4 8 3 4 0 5 1 9 4 5 7 6 3 7 0 5 9 7 5 9 7 4 2 1 9 0 7 5 3 3 6 3 9 6 9 5 0 1 5 5 8 3\n",
            " 3 6 2 6 5]\n",
            "\n",
            "The r^2 score is\t 0.9704127730521286\n",
            "\n",
            "The mean absolute error on the training set is\t 0.04083147735708983\n",
            "\n",
            "The predicted y values for the test set are\t [6 9 3 7 2 2 5 2 5 2 1 9 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 9 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 5 5 4 7 0 4 5 5 9 9 0 2 3 8 0 6 4 4\n",
            " 9 1 2 8 3 5 2 9 0 4 4 4 3 5 3 1 3 5 9 4 2 7 7 4 4 1 9 2 7 8 7 2 6 9 4 0 7 2 7 5 8 7 5 7 9 0 6 6 4 2 8 0 9 4 6 9 9 6 9 0 5 5 6 6 0 6 4 3 9 3 8 7 2 9 0 4 5 8 6 5 9 9 8 4 2 1 3 7 7\n",
            " 2 2 3 9 8 0 3 2 2 5 6 9 9 4 1 2 4 2 3 6 4 8 5 9 5 7 8 9 4 8 1 5 4 4 9 6 1 8 6 0 4 5 2 7 1 6 4 5 6 0 3 2 3 6 7 1 9 1 4 7 6 5 8 5 5 1 5 2 8 8 9 9 7 6 2 2 2 3 4 8 8 3 6 0 9 7 7 0 1\n",
            " 0 4 5 1 5 3 6 0 4 1 0 0 3 6 5 9 7 3 5 5 9 9 8 5 3 3 2 0 5 8 3 4 0 2 4 6 4 3 4 5 0 5 2 1 3 1 4 1 1 7 0 1 5 2 1 2 8 7 0 6 4 8 8 5 1 8 4 5 8 7 9 8 6 0 6 2 0 7 9 8 9 5 2 7 7 1 8 7 4\n",
            " 3 8 3 5 6 0 0 3 0 5 0 0 4 1 2 8 4 5 9 6 3 1 8 8 4 2 3 8 9 8 8 5 0 6 3 3 7 1 6 4 1 2 1 1 6 4 7 4 8 3 4 0 5 1 9 4 5 7 6 3 7 0 5 9 7 5 9 7 4 2 1 9 0 7 5 8 3 6 3 9 6 9 5 0 1 5 5 8 3\n",
            " 3 6 2 6 5]\n",
            "The real y values for the test set are\t [6 9 3 7 2 1 5 2 5 2 1 9 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 9 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 5 5 4 7 0 4 5 5 9 9 0 2 3 8 0 6 4 4\n",
            " 9 1 2 8 3 5 2 9 0 4 4 4 3 5 3 1 3 5 9 4 2 7 7 4 4 1 9 2 7 8 7 2 6 9 4 0 7 2 7 5 8 7 5 7 7 0 6 6 4 2 8 0 9 4 6 9 9 6 9 0 3 5 6 6 0 6 4 3 9 3 9 7 2 9 0 4 5 3 6 5 9 9 8 4 2 1 3 7 7\n",
            " 2 2 3 9 8 0 3 2 2 5 6 9 9 4 1 5 4 2 3 6 4 8 5 9 5 7 8 9 4 8 1 5 4 4 9 6 1 8 6 0 4 5 2 7 4 6 4 5 6 0 3 2 3 6 7 1 5 1 4 7 6 8 8 5 5 1 6 2 8 8 9 9 7 6 2 2 2 3 4 8 8 3 6 0 9 7 7 0 1\n",
            " 0 4 5 1 5 3 6 0 4 1 0 0 3 6 5 9 7 3 5 5 9 9 8 5 3 3 2 0 5 8 3 4 0 2 4 6 4 3 4 5 0 5 2 1 3 1 4 1 1 7 0 1 5 2 1 2 8 7 0 6 4 8 8 5 1 8 4 5 8 7 9 8 5 0 6 2 0 7 9 8 9 5 2 7 7 1 8 7 4\n",
            " 3 8 3 5 6 0 0 3 0 5 0 0 4 1 2 8 4 5 9 6 3 1 8 8 4 2 3 8 9 8 8 5 0 6 3 3 7 1 6 4 1 2 1 1 6 4 7 4 8 3 4 0 5 1 9 4 5 7 6 3 7 0 5 9 7 5 9 7 4 2 1 9 0 7 5 3 3 6 3 9 6 9 5 0 1 5 5 8 3\n",
            " 3 6 2 6 5]\n",
            "\n",
            "The mean absolute error on the test set is\t 0.06888888888888889 \n",
            "\n",
            "------------\n",
            "Using the multipoint gradient variant\n",
            "-------------\n",
            "Initial starting points:  (65, 10)\n",
            "Gradient descent failed\n",
            "Initial starting points:  (65, 10)\n",
            "Gradient descent failed\n",
            "\n",
            "Fitting time is 14.009025812149048\n",
            "\n",
            "Predicting the model...\n",
            "\n",
            "Prediction time is 0.0007822513580322266\n",
            "Accuracy: 0.9444444444444444\n",
            "Mean of predictions:  [6 9 3 7 2 1 5 2 5 7 1 9 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 9 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 9 5 4 7 0 1 5 5 9 9 0 2 3 8 0 6 4 4\n",
            " 9 1 2 8 3 5 2 9 4 4 4 4 3 5 3 1 3 5 9 4 2 7 7 4 4 1 9 2 7 8 7 2 6 9 4 2 7 2 7 5 8 7 5 7 9 0 6 6 4 2 8 0 9 4 6 9 9 6 9 0 5 5 6 6 0 6 4 3 9 3 8 7 2 9 0 0 5 8 6 5 9 9 2 4 2 1 3 7 7\n",
            " 2 2 3 9 8 0 3 2 3 5 6 9 9 4 1 5 4 2 3 6 4 8 5 9 5 7 8 9 4 8 1 5 4 4 9 6 1 8 6 0 4 5 2 7 1 6 4 5 6 0 3 2 3 6 7 1 5 1 4 7 6 0 8 5 5 1 0 2 8 8 9 3 7 4 2 2 2 3 4 8 8 3 6 0 9 7 9 0 1\n",
            " 0 4 5 1 5 3 6 0 4 1 0 0 3 6 5 9 7 3 5 5 9 9 8 5 3 3 2 0 5 8 3 4 0 2 4 6 4 3 4 5 0 5 2 1 3 1 4 1 1 7 0 1 5 2 1 2 8 7 0 6 4 8 8 5 1 8 4 5 8 7 9 8 6 0 6 2 0 7 9 8 9 5 2 7 7 1 8 7 4\n",
            " 3 2 3 5 6 0 0 3 0 5 0 0 4 1 2 8 4 5 9 6 3 1 8 8 4 2 3 8 9 8 8 4 0 6 3 3 7 1 6 4 1 2 1 1 6 4 4 4 8 3 4 0 5 1 9 4 5 7 6 3 7 0 4 9 7 5 9 7 4 2 1 9 0 7 5 8 3 6 3 9 6 9 5 0 1 5 5 8 7\n",
            " 3 6 2 6 5]\n",
            "\n",
            "Real values:  [6 9 3 7 2 1 5 2 5 2 1 9 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 9 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 5 5 4 7 0 4 5 5 9 9 0 2 3 8 0 6 4 4\n",
            " 9 1 2 8 3 5 2 9 0 4 4 4 3 5 3 1 3 5 9 4 2 7 7 4 4 1 9 2 7 8 7 2 6 9 4 0 7 2 7 5 8 7 5 7 7 0 6 6 4 2 8 0 9 4 6 9 9 6 9 0 3 5 6 6 0 6 4 3 9 3 9 7 2 9 0 4 5 3 6 5 9 9 8 4 2 1 3 7 7\n",
            " 2 2 3 9 8 0 3 2 2 5 6 9 9 4 1 5 4 2 3 6 4 8 5 9 5 7 8 9 4 8 1 5 4 4 9 6 1 8 6 0 4 5 2 7 4 6 4 5 6 0 3 2 3 6 7 1 5 1 4 7 6 8 8 5 5 1 6 2 8 8 9 9 7 6 2 2 2 3 4 8 8 3 6 0 9 7 7 0 1\n",
            " 0 4 5 1 5 3 6 0 4 1 0 0 3 6 5 9 7 3 5 5 9 9 8 5 3 3 2 0 5 8 3 4 0 2 4 6 4 3 4 5 0 5 2 1 3 1 4 1 1 7 0 1 5 2 1 2 8 7 0 6 4 8 8 5 1 8 4 5 8 7 9 8 5 0 6 2 0 7 9 8 9 5 2 7 7 1 8 7 4\n",
            " 3 8 3 5 6 0 0 3 0 5 0 0 4 1 2 8 4 5 9 6 3 1 8 8 4 2 3 8 9 8 8 5 0 6 3 3 7 1 6 4 1 2 1 1 6 4 7 4 8 3 4 0 5 1 9 4 5 7 6 3 7 0 5 9 7 5 9 7 4 2 1 9 0 7 5 3 3 6 3 9 6 9 5 0 1 5 5 8 3\n",
            " 3 6 2 6 5]\n",
            "\n",
            "The r^2 score is\t 0.9301634823392584\n",
            "\n",
            "The mean absolute error on the training set is\t 0.10987379361544172\n",
            "\n",
            "The predicted y values for the test set are\t [6 9 3 7 2 1 5 2 5 7 1 9 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 9 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 9 5 4 7 0 1 5 5 9 9 0 2 3 8 0 6 4 4\n",
            " 9 1 2 8 3 5 2 9 4 4 4 4 3 5 3 1 3 5 9 4 2 7 7 4 4 1 9 2 7 8 7 2 6 9 4 2 7 2 7 5 8 7 5 7 9 0 6 6 4 2 8 0 9 4 6 9 9 6 9 0 5 5 6 6 0 6 4 3 9 3 8 7 2 9 0 0 5 8 6 5 9 9 2 4 2 1 3 7 7\n",
            " 2 2 3 9 8 0 3 2 3 5 6 9 9 4 1 5 4 2 3 6 4 8 5 9 5 7 8 9 4 8 1 5 4 4 9 6 1 8 6 0 4 5 2 7 1 6 4 5 6 0 3 2 3 6 7 1 5 1 4 7 6 0 8 5 5 1 0 2 8 8 9 3 7 4 2 2 2 3 4 8 8 3 6 0 9 7 9 0 1\n",
            " 0 4 5 1 5 3 6 0 4 1 0 0 3 6 5 9 7 3 5 5 9 9 8 5 3 3 2 0 5 8 3 4 0 2 4 6 4 3 4 5 0 5 2 1 3 1 4 1 1 7 0 1 5 2 1 2 8 7 0 6 4 8 8 5 1 8 4 5 8 7 9 8 6 0 6 2 0 7 9 8 9 5 2 7 7 1 8 7 4\n",
            " 3 2 3 5 6 0 0 3 0 5 0 0 4 1 2 8 4 5 9 6 3 1 8 8 4 2 3 8 9 8 8 4 0 6 3 3 7 1 6 4 1 2 1 1 6 4 4 4 8 3 4 0 5 1 9 4 5 7 6 3 7 0 4 9 7 5 9 7 4 2 1 9 0 7 5 8 3 6 3 9 6 9 5 0 1 5 5 8 7\n",
            " 3 6 2 6 5]\n",
            "The real y values for the test set are\t [6 9 3 7 2 1 5 2 5 2 1 9 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 9 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 5 5 4 7 0 4 5 5 9 9 0 2 3 8 0 6 4 4\n",
            " 9 1 2 8 3 5 2 9 0 4 4 4 3 5 3 1 3 5 9 4 2 7 7 4 4 1 9 2 7 8 7 2 6 9 4 0 7 2 7 5 8 7 5 7 7 0 6 6 4 2 8 0 9 4 6 9 9 6 9 0 3 5 6 6 0 6 4 3 9 3 9 7 2 9 0 4 5 3 6 5 9 9 8 4 2 1 3 7 7\n",
            " 2 2 3 9 8 0 3 2 2 5 6 9 9 4 1 5 4 2 3 6 4 8 5 9 5 7 8 9 4 8 1 5 4 4 9 6 1 8 6 0 4 5 2 7 4 6 4 5 6 0 3 2 3 6 7 1 5 1 4 7 6 8 8 5 5 1 6 2 8 8 9 9 7 6 2 2 2 3 4 8 8 3 6 0 9 7 7 0 1\n",
            " 0 4 5 1 5 3 6 0 4 1 0 0 3 6 5 9 7 3 5 5 9 9 8 5 3 3 2 0 5 8 3 4 0 2 4 6 4 3 4 5 0 5 2 1 3 1 4 1 1 7 0 1 5 2 1 2 8 7 0 6 4 8 8 5 1 8 4 5 8 7 9 8 5 0 6 2 0 7 9 8 9 5 2 7 7 1 8 7 4\n",
            " 3 8 3 5 6 0 0 3 0 5 0 0 4 1 2 8 4 5 9 6 3 1 8 8 4 2 3 8 9 8 8 5 0 6 3 3 7 1 6 4 1 2 1 1 6 4 7 4 8 3 4 0 5 1 9 4 5 7 6 3 7 0 5 9 7 5 9 7 4 2 1 9 0 7 5 3 3 6 3 9 6 9 5 0 1 5 5 8 3\n",
            " 3 6 2 6 5]\n",
            "\n",
            "The mean absolute error on the test set is\t 0.19333333333333333 \n",
            "\n",
            "------------\n",
            "Using the lrSchedual gradient variant\n",
            "-------------\n",
            "Epoch  0\n",
            "Initial starting points:  (65, 10)\n",
            "Gradient descent failed\n",
            "Initial starting points:  (65, 10)\n",
            "Gradient descent failed\n",
            "Epoch  1\n",
            "Initial starting points:  (65, 10)\n",
            "Gradient descent failed\n",
            "Initial starting points:  (65, 10)\n",
            "Gradient descent failed\n",
            "Finding the best parameters at the epoch of  0\n",
            "\n",
            "Fitting time is 18.877392768859863\n",
            "\n",
            "Predicting the model...\n",
            "\n",
            "Prediction time is 0.0007135868072509766\n",
            "Accuracy: 0.9511111111111111\n",
            "Mean of predictions:  [6 9 3 7 2 2 5 2 5 2 1 9 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 9 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 9 5 4 7 0 4 5 5 9 9 0 2 3 8 0 6 4 4\n",
            " 9 1 2 8 3 5 2 9 0 4 4 4 3 5 3 1 3 5 9 4 2 7 7 4 4 1 9 2 7 8 7 2 6 9 4 0 7 2 7 5 8 7 5 7 9 0 6 6 4 2 8 0 9 4 6 9 9 6 9 0 5 5 6 6 0 6 4 3 9 3 8 7 2 9 0 1 5 3 6 5 9 9 8 4 2 1 3 7 7\n",
            " 2 2 3 9 8 0 3 2 2 5 6 9 9 4 1 2 4 2 3 6 4 8 5 9 5 7 8 9 4 8 1 5 4 4 9 6 1 8 6 0 4 5 2 7 1 6 4 5 6 0 3 2 3 6 7 1 5 1 4 7 6 5 8 5 5 1 0 2 8 8 9 1 7 1 2 2 2 3 4 8 8 3 6 0 9 7 7 0 1\n",
            " 0 4 5 1 5 3 6 0 4 1 0 0 3 6 5 9 7 3 5 5 9 9 8 5 3 3 2 0 5 8 3 4 0 2 4 6 4 3 4 5 0 5 2 1 3 1 4 1 1 7 0 1 5 2 1 2 8 7 0 6 4 8 8 5 1 8 4 5 8 7 9 8 6 0 6 2 0 7 9 1 9 5 2 7 7 9 8 7 4\n",
            " 3 7 3 5 6 0 0 3 0 5 0 0 4 1 1 8 4 5 9 6 3 1 8 8 4 2 3 8 9 8 8 5 0 6 3 3 7 1 6 4 1 2 1 1 6 4 9 4 8 3 4 0 5 1 3 4 5 7 6 3 7 0 5 9 9 5 9 7 4 2 2 9 0 7 5 8 3 6 3 9 6 9 5 0 1 5 5 8 3\n",
            " 3 6 2 6 5]\n",
            "\n",
            "Real values:  [6 9 3 7 2 1 5 2 5 2 1 9 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 9 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 5 5 4 7 0 4 5 5 9 9 0 2 3 8 0 6 4 4\n",
            " 9 1 2 8 3 5 2 9 0 4 4 4 3 5 3 1 3 5 9 4 2 7 7 4 4 1 9 2 7 8 7 2 6 9 4 0 7 2 7 5 8 7 5 7 7 0 6 6 4 2 8 0 9 4 6 9 9 6 9 0 3 5 6 6 0 6 4 3 9 3 9 7 2 9 0 4 5 3 6 5 9 9 8 4 2 1 3 7 7\n",
            " 2 2 3 9 8 0 3 2 2 5 6 9 9 4 1 5 4 2 3 6 4 8 5 9 5 7 8 9 4 8 1 5 4 4 9 6 1 8 6 0 4 5 2 7 4 6 4 5 6 0 3 2 3 6 7 1 5 1 4 7 6 8 8 5 5 1 6 2 8 8 9 9 7 6 2 2 2 3 4 8 8 3 6 0 9 7 7 0 1\n",
            " 0 4 5 1 5 3 6 0 4 1 0 0 3 6 5 9 7 3 5 5 9 9 8 5 3 3 2 0 5 8 3 4 0 2 4 6 4 3 4 5 0 5 2 1 3 1 4 1 1 7 0 1 5 2 1 2 8 7 0 6 4 8 8 5 1 8 4 5 8 7 9 8 5 0 6 2 0 7 9 8 9 5 2 7 7 1 8 7 4\n",
            " 3 8 3 5 6 0 0 3 0 5 0 0 4 1 2 8 4 5 9 6 3 1 8 8 4 2 3 8 9 8 8 5 0 6 3 3 7 1 6 4 1 2 1 1 6 4 7 4 8 3 4 0 5 1 9 4 5 7 6 3 7 0 5 9 7 5 9 7 4 2 1 9 0 7 5 3 3 6 3 9 6 9 5 0 1 5 5 8 3\n",
            " 3 6 2 6 5]\n",
            "\n",
            "The r^2 score is\t 0.9394039376022573\n",
            "\n",
            "The mean absolute error on the training set is\t 0.08760207869339272\n",
            "\n",
            "The predicted y values for the test set are\t [6 9 3 7 2 2 5 2 5 2 1 9 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 9 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 9 5 4 7 0 4 5 5 9 9 0 2 3 8 0 6 4 4\n",
            " 9 1 2 8 3 5 2 9 0 4 4 4 3 5 3 1 3 5 9 4 2 7 7 4 4 1 9 2 7 8 7 2 6 9 4 0 7 2 7 5 8 7 5 7 9 0 6 6 4 2 8 0 9 4 6 9 9 6 9 0 5 5 6 6 0 6 4 3 9 3 8 7 2 9 0 1 5 3 6 5 9 9 8 4 2 1 3 7 7\n",
            " 2 2 3 9 8 0 3 2 2 5 6 9 9 4 1 2 4 2 3 6 4 8 5 9 5 7 8 9 4 8 1 5 4 4 9 6 1 8 6 0 4 5 2 7 1 6 4 5 6 0 3 2 3 6 7 1 5 1 4 7 6 5 8 5 5 1 0 2 8 8 9 1 7 1 2 2 2 3 4 8 8 3 6 0 9 7 7 0 1\n",
            " 0 4 5 1 5 3 6 0 4 1 0 0 3 6 5 9 7 3 5 5 9 9 8 5 3 3 2 0 5 8 3 4 0 2 4 6 4 3 4 5 0 5 2 1 3 1 4 1 1 7 0 1 5 2 1 2 8 7 0 6 4 8 8 5 1 8 4 5 8 7 9 8 6 0 6 2 0 7 9 1 9 5 2 7 7 9 8 7 4\n",
            " 3 7 3 5 6 0 0 3 0 5 0 0 4 1 1 8 4 5 9 6 3 1 8 8 4 2 3 8 9 8 8 5 0 6 3 3 7 1 6 4 1 2 1 1 6 4 9 4 8 3 4 0 5 1 3 4 5 7 6 3 7 0 5 9 9 5 9 7 4 2 2 9 0 7 5 8 3 6 3 9 6 9 5 0 1 5 5 8 3\n",
            " 3 6 2 6 5]\n",
            "The real y values for the test set are\t [6 9 3 7 2 1 5 2 5 2 1 9 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 9 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 5 5 4 7 0 4 5 5 9 9 0 2 3 8 0 6 4 4\n",
            " 9 1 2 8 3 5 2 9 0 4 4 4 3 5 3 1 3 5 9 4 2 7 7 4 4 1 9 2 7 8 7 2 6 9 4 0 7 2 7 5 8 7 5 7 7 0 6 6 4 2 8 0 9 4 6 9 9 6 9 0 3 5 6 6 0 6 4 3 9 3 9 7 2 9 0 4 5 3 6 5 9 9 8 4 2 1 3 7 7\n",
            " 2 2 3 9 8 0 3 2 2 5 6 9 9 4 1 5 4 2 3 6 4 8 5 9 5 7 8 9 4 8 1 5 4 4 9 6 1 8 6 0 4 5 2 7 4 6 4 5 6 0 3 2 3 6 7 1 5 1 4 7 6 8 8 5 5 1 6 2 8 8 9 9 7 6 2 2 2 3 4 8 8 3 6 0 9 7 7 0 1\n",
            " 0 4 5 1 5 3 6 0 4 1 0 0 3 6 5 9 7 3 5 5 9 9 8 5 3 3 2 0 5 8 3 4 0 2 4 6 4 3 4 5 0 5 2 1 3 1 4 1 1 7 0 1 5 2 1 2 8 7 0 6 4 8 8 5 1 8 4 5 8 7 9 8 5 0 6 2 0 7 9 8 9 5 2 7 7 1 8 7 4\n",
            " 3 8 3 5 6 0 0 3 0 5 0 0 4 1 2 8 4 5 9 6 3 1 8 8 4 2 3 8 9 8 8 5 0 6 3 3 7 1 6 4 1 2 1 1 6 4 7 4 8 3 4 0 5 1 9 4 5 7 6 3 7 0 5 9 7 5 9 7 4 2 1 9 0 7 5 3 3 6 3 9 6 9 5 0 1 5 5 8 3\n",
            " 3 6 2 6 5]\n",
            "\n",
            "The mean absolute error on the test set is\t 0.16666666666666666 \n",
            "\n",
            "\n",
            "Using numerical gradient method\n",
            "-------------------------------------------------------------\n",
            "------------\n",
            "Using the simple gradient variant\n",
            "-------------\n",
            "Gradient descent took 0 iterations to converge\n",
            "\n",
            "Fitting time is 0.04336237907409668\n",
            "\n",
            "Predicting the model...\n",
            "\n",
            "Prediction time is 0.0007185935974121094\n",
            "Accuracy: 0.06222222222222222\n",
            "Mean of predictions:  [0 0 6 5 7 5 4 4 7 7 4 2 7 3 0 4 0 2 4 5 2 6 6 2 7 7 0 5 0 6 2 0 0 0 0 0 6 5 2 0 0 6 2 6 7 5 6 6 7 7 5 4 0 5 0 6 0 7 0 5 7 5 4 4 2 6 7 6 4 0 6 0 7 5 4 3 2 4 6 6 0 0 4 6 4 3 0 0 0\n",
            " 6 7 4 4 6 6 0 6 0 5 5 2 6 0 4 2 4 7 2 2 0 4 2 2 2 0 6 4 5 6 4 4 0 6 0 0 0 4 4 4 6 2 7 7 4 5 0 7 5 4 4 0 0 5 0 4 3 7 6 0 5 7 7 0 6 5 5 4 6 6 4 4 7 6 0 0 7 4 0 7 4 6 4 5 7 7 6 4 4\n",
            " 7 7 6 3 6 6 6 7 0 4 0 6 3 7 7 7 0 4 6 0 5 6 7 6 4 4 7 0 5 4 7 7 5 5 0 0 2 6 0 3 0 7 4 2 2 0 3 7 7 0 6 4 6 0 1 6 6 2 2 2 0 5 4 5 7 2 0 4 7 6 2 7 2 7 4 7 0 6 5 4 5 6 0 6 4 4 4 0 7\n",
            " 3 0 6 4 7 6 0 3 0 0 3 3 0 0 6 0 2 7 7 6 0 0 2 4 6 0 4 6 7 4 6 2 3 4 2 0 2 6 5 6 0 6 7 2 6 2 3 7 2 4 0 7 5 7 2 4 2 4 3 7 5 4 4 7 7 4 5 5 7 5 6 7 6 3 7 7 0 5 0 7 0 5 4 4 2 3 4 4 0\n",
            " 6 4 6 6 0 6 0 6 6 7 0 3 5 0 7 6 2 2 4 0 6 7 4 7 5 4 0 4 0 4 5 4 3 0 0 6 5 7 0 0 5 4 2 2 0 7 4 0 7 6 0 0 7 6 6 0 5 4 0 6 4 6 0 0 5 5 0 5 3 4 5 6 7 4 7 4 6 0 6 0 5 6 5 3 7 6 5 6 6\n",
            " 6 5 6 7 7]\n",
            "\n",
            "Real values:  [6 9 3 7 2 1 5 2 5 2 1 9 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 9 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 5 5 4 7 0 4 5 5 9 9 0 2 3 8 0 6 4 4\n",
            " 9 1 2 8 3 5 2 9 0 4 4 4 3 5 3 1 3 5 9 4 2 7 7 4 4 1 9 2 7 8 7 2 6 9 4 0 7 2 7 5 8 7 5 7 7 0 6 6 4 2 8 0 9 4 6 9 9 6 9 0 3 5 6 6 0 6 4 3 9 3 9 7 2 9 0 4 5 3 6 5 9 9 8 4 2 1 3 7 7\n",
            " 2 2 3 9 8 0 3 2 2 5 6 9 9 4 1 5 4 2 3 6 4 8 5 9 5 7 8 9 4 8 1 5 4 4 9 6 1 8 6 0 4 5 2 7 4 6 4 5 6 0 3 2 3 6 7 1 5 1 4 7 6 8 8 5 5 1 6 2 8 8 9 9 7 6 2 2 2 3 4 8 8 3 6 0 9 7 7 0 1\n",
            " 0 4 5 1 5 3 6 0 4 1 0 0 3 6 5 9 7 3 5 5 9 9 8 5 3 3 2 0 5 8 3 4 0 2 4 6 4 3 4 5 0 5 2 1 3 1 4 1 1 7 0 1 5 2 1 2 8 7 0 6 4 8 8 5 1 8 4 5 8 7 9 8 5 0 6 2 0 7 9 8 9 5 2 7 7 1 8 7 4\n",
            " 3 8 3 5 6 0 0 3 0 5 0 0 4 1 2 8 4 5 9 6 3 1 8 8 4 2 3 8 9 8 8 5 0 6 3 3 7 1 6 4 1 2 1 1 6 4 7 4 8 3 4 0 5 1 9 4 5 7 6 3 7 0 5 9 7 5 9 7 4 2 1 9 0 7 5 3 3 6 3 9 6 9 5 0 1 5 5 8 3\n",
            " 3 6 2 6 5]\n",
            "\n",
            "The r^2 score is\t -0.6367156384586787\n",
            "\n",
            "The mean absolute error on the training set is\t 3.129175946547884\n",
            "\n",
            "The predicted y values for the test set are\t [0 0 6 5 7 5 4 4 7 7 4 2 7 3 0 4 0 2 4 5 2 6 6 2 7 7 0 5 0 6 2 0 0 0 0 0 6 5 2 0 0 6 2 6 7 5 6 6 7 7 5 4 0 5 0 6 0 7 0 5 7 5 4 4 2 6 7 6 4 0 6 0 7 5 4 3 2 4 6 6 0 0 4 6 4 3 0 0 0\n",
            " 6 7 4 4 6 6 0 6 0 5 5 2 6 0 4 2 4 7 2 2 0 4 2 2 2 0 6 4 5 6 4 4 0 6 0 0 0 4 4 4 6 2 7 7 4 5 0 7 5 4 4 0 0 5 0 4 3 7 6 0 5 7 7 0 6 5 5 4 6 6 4 4 7 6 0 0 7 4 0 7 4 6 4 5 7 7 6 4 4\n",
            " 7 7 6 3 6 6 6 7 0 4 0 6 3 7 7 7 0 4 6 0 5 6 7 6 4 4 7 0 5 4 7 7 5 5 0 0 2 6 0 3 0 7 4 2 2 0 3 7 7 0 6 4 6 0 1 6 6 2 2 2 0 5 4 5 7 2 0 4 7 6 2 7 2 7 4 7 0 6 5 4 5 6 0 6 4 4 4 0 7\n",
            " 3 0 6 4 7 6 0 3 0 0 3 3 0 0 6 0 2 7 7 6 0 0 2 4 6 0 4 6 7 4 6 2 3 4 2 0 2 6 5 6 0 6 7 2 6 2 3 7 2 4 0 7 5 7 2 4 2 4 3 7 5 4 4 7 7 4 5 5 7 5 6 7 6 3 7 7 0 5 0 7 0 5 4 4 2 3 4 4 0\n",
            " 6 4 6 6 0 6 0 6 6 7 0 3 5 0 7 6 2 2 4 0 6 7 4 7 5 4 0 4 0 4 5 4 3 0 0 6 5 7 0 0 5 4 2 2 0 7 4 0 7 6 0 0 7 6 6 0 5 4 0 6 4 6 0 0 5 5 0 5 3 4 5 6 7 4 7 4 6 0 6 0 5 6 5 3 7 6 5 6 6\n",
            " 6 5 6 7 7]\n",
            "The real y values for the test set are\t [6 9 3 7 2 1 5 2 5 2 1 9 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 9 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 5 5 4 7 0 4 5 5 9 9 0 2 3 8 0 6 4 4\n",
            " 9 1 2 8 3 5 2 9 0 4 4 4 3 5 3 1 3 5 9 4 2 7 7 4 4 1 9 2 7 8 7 2 6 9 4 0 7 2 7 5 8 7 5 7 7 0 6 6 4 2 8 0 9 4 6 9 9 6 9 0 3 5 6 6 0 6 4 3 9 3 9 7 2 9 0 4 5 3 6 5 9 9 8 4 2 1 3 7 7\n",
            " 2 2 3 9 8 0 3 2 2 5 6 9 9 4 1 5 4 2 3 6 4 8 5 9 5 7 8 9 4 8 1 5 4 4 9 6 1 8 6 0 4 5 2 7 4 6 4 5 6 0 3 2 3 6 7 1 5 1 4 7 6 8 8 5 5 1 6 2 8 8 9 9 7 6 2 2 2 3 4 8 8 3 6 0 9 7 7 0 1\n",
            " 0 4 5 1 5 3 6 0 4 1 0 0 3 6 5 9 7 3 5 5 9 9 8 5 3 3 2 0 5 8 3 4 0 2 4 6 4 3 4 5 0 5 2 1 3 1 4 1 1 7 0 1 5 2 1 2 8 7 0 6 4 8 8 5 1 8 4 5 8 7 9 8 5 0 6 2 0 7 9 8 9 5 2 7 7 1 8 7 4\n",
            " 3 8 3 5 6 0 0 3 0 5 0 0 4 1 2 8 4 5 9 6 3 1 8 8 4 2 3 8 9 8 8 5 0 6 3 3 7 1 6 4 1 2 1 1 6 4 7 4 8 3 4 0 5 1 9 4 5 7 6 3 7 0 5 9 7 5 9 7 4 2 1 9 0 7 5 3 3 6 3 9 6 9 5 0 1 5 5 8 3\n",
            " 3 6 2 6 5]\n",
            "\n",
            "The mean absolute error on the test set is\t 3.1822222222222223 \n",
            "\n",
            "------------\n",
            "Using the multipoint gradient variant\n",
            "-------------\n",
            "Initial starting points:  (65, 10)\n",
            "Gradient descent took 0 iterations to converge\n",
            "Initial starting points:  (65, 10)\n",
            "Gradient descent took 0 iterations to converge\n",
            "\n",
            "Fitting time is 0.08947324752807617\n",
            "\n",
            "Predicting the model...\n",
            "\n",
            "Prediction time is 0.0007221698760986328\n",
            "Accuracy: 0.11555555555555555\n",
            "Mean of predictions:  [4 9 7 4 4 4 5 4 4 4 6 6 4 7 7 4 6 4 4 4 6 4 4 4 4 4 4 4 6 4 6 4 4 6 6 6 9 6 4 4 4 6 4 4 6 4 6 6 6 5 4 6 4 4 6 6 4 4 4 4 4 6 5 4 6 4 4 6 4 7 7 4 6 4 4 4 7 4 4 4 7 6 4 6 4 7 4 6 7\n",
            " 6 4 4 4 6 4 4 9 6 6 4 5 4 4 6 6 4 4 5 6 4 7 4 6 6 6 4 4 5 4 7 4 6 9 7 7 4 4 4 4 6 4 4 4 5 6 4 6 6 4 4 4 6 5 6 6 6 6 4 6 7 5 4 6 4 4 4 4 4 7 4 4 4 4 6 7 4 6 4 4 4 7 4 6 4 4 7 4 8\n",
            " 4 4 4 4 4 7 4 4 4 4 4 4 4 6 4 4 6 4 4 4 6 4 4 4 5 7 4 7 6 4 6 4 6 6 7 6 4 6 7 6 6 4 4 4 7 6 6 4 6 6 4 4 4 4 4 6 4 4 7 7 6 4 4 7 5 6 7 4 4 4 4 4 4 7 4 4 4 7 4 6 4 6 4 6 7 7 4 7 4\n",
            " 7 6 4 4 4 4 6 7 5 4 4 7 4 4 4 7 7 6 7 4 4 4 4 4 6 7 4 4 4 4 7 6 7 4 5 6 6 7 4 7 4 7 4 4 4 6 6 4 4 6 4 4 4 4 4 4 6 4 6 6 7 4 6 6 4 4 4 4 4 6 7 4 6 9 6 4 6 5 4 6 4 5 4 6 7 9 6 4 5\n",
            " 6 4 4 4 6 7 6 4 4 4 6 4 4 6 4 4 2 4 4 6 4 4 6 4 4 4 7 6 6 4 4 5 4 7 4 4 4 4 6 6 4 4 4 4 7 6 7 6 4 4 6 6 4 6 4 7 4 4 4 4 7 4 6 4 5 7 4 4 6 4 4 4 4 4 4 6 7 4 7 4 6 4 4 7 6 4 4 4 4\n",
            " 6 4 4 7 4]\n",
            "\n",
            "Real values:  [6 9 3 7 2 1 5 2 5 2 1 9 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 9 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 5 5 4 7 0 4 5 5 9 9 0 2 3 8 0 6 4 4\n",
            " 9 1 2 8 3 5 2 9 0 4 4 4 3 5 3 1 3 5 9 4 2 7 7 4 4 1 9 2 7 8 7 2 6 9 4 0 7 2 7 5 8 7 5 7 7 0 6 6 4 2 8 0 9 4 6 9 9 6 9 0 3 5 6 6 0 6 4 3 9 3 9 7 2 9 0 4 5 3 6 5 9 9 8 4 2 1 3 7 7\n",
            " 2 2 3 9 8 0 3 2 2 5 6 9 9 4 1 5 4 2 3 6 4 8 5 9 5 7 8 9 4 8 1 5 4 4 9 6 1 8 6 0 4 5 2 7 4 6 4 5 6 0 3 2 3 6 7 1 5 1 4 7 6 8 8 5 5 1 6 2 8 8 9 9 7 6 2 2 2 3 4 8 8 3 6 0 9 7 7 0 1\n",
            " 0 4 5 1 5 3 6 0 4 1 0 0 3 6 5 9 7 3 5 5 9 9 8 5 3 3 2 0 5 8 3 4 0 2 4 6 4 3 4 5 0 5 2 1 3 1 4 1 1 7 0 1 5 2 1 2 8 7 0 6 4 8 8 5 1 8 4 5 8 7 9 8 5 0 6 2 0 7 9 8 9 5 2 7 7 1 8 7 4\n",
            " 3 8 3 5 6 0 0 3 0 5 0 0 4 1 2 8 4 5 9 6 3 1 8 8 4 2 3 8 9 8 8 5 0 6 3 3 7 1 6 4 1 2 1 1 6 4 7 4 8 3 4 0 5 1 9 4 5 7 6 3 7 0 5 9 7 5 9 7 4 2 1 9 0 7 5 3 3 6 3 9 6 9 5 0 1 5 5 8 3\n",
            " 3 6 2 6 5]\n",
            "\n",
            "The r^2 score is\t -0.24141962437135112\n",
            "\n",
            "The mean absolute error on the training set is\t 2.684484038604306\n",
            "\n",
            "The predicted y values for the test set are\t [4 9 7 4 4 4 5 4 4 4 6 6 4 7 7 4 6 4 4 4 6 4 4 4 4 4 4 4 6 4 6 4 4 6 6 6 9 6 4 4 4 6 4 4 6 4 6 6 6 5 4 6 4 4 6 6 4 4 4 4 4 6 5 4 6 4 4 6 4 7 7 4 6 4 4 4 7 4 4 4 7 6 4 6 4 7 4 6 7\n",
            " 6 4 4 4 6 4 4 9 6 6 4 5 4 4 6 6 4 4 5 6 4 7 4 6 6 6 4 4 5 4 7 4 6 9 7 7 4 4 4 4 6 4 4 4 5 6 4 6 6 4 4 4 6 5 6 6 6 6 4 6 7 5 4 6 4 4 4 4 4 7 4 4 4 4 6 7 4 6 4 4 4 7 4 6 4 4 7 4 8\n",
            " 4 4 4 4 4 7 4 4 4 4 4 4 4 6 4 4 6 4 4 4 6 4 4 4 5 7 4 7 6 4 6 4 6 6 7 6 4 6 7 6 6 4 4 4 7 6 6 4 6 6 4 4 4 4 4 6 4 4 7 7 6 4 4 7 5 6 7 4 4 4 4 4 4 7 4 4 4 7 4 6 4 6 4 6 7 7 4 7 4\n",
            " 7 6 4 4 4 4 6 7 5 4 4 7 4 4 4 7 7 6 7 4 4 4 4 4 6 7 4 4 4 4 7 6 7 4 5 6 6 7 4 7 4 7 4 4 4 6 6 4 4 6 4 4 4 4 4 4 6 4 6 6 7 4 6 6 4 4 4 4 4 6 7 4 6 9 6 4 6 5 4 6 4 5 4 6 7 9 6 4 5\n",
            " 6 4 4 4 6 7 6 4 4 4 6 4 4 6 4 4 2 4 4 6 4 4 6 4 4 4 7 6 6 4 4 5 4 7 4 4 4 4 6 6 4 4 4 4 7 6 7 6 4 4 6 6 4 6 4 7 4 4 4 4 7 4 6 4 5 7 4 4 6 4 4 4 4 4 4 6 7 4 7 4 6 4 4 7 6 4 4 4 4\n",
            " 6 4 4 7 4]\n",
            "The real y values for the test set are\t [6 9 3 7 2 1 5 2 5 2 1 9 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 9 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 5 5 4 7 0 4 5 5 9 9 0 2 3 8 0 6 4 4\n",
            " 9 1 2 8 3 5 2 9 0 4 4 4 3 5 3 1 3 5 9 4 2 7 7 4 4 1 9 2 7 8 7 2 6 9 4 0 7 2 7 5 8 7 5 7 7 0 6 6 4 2 8 0 9 4 6 9 9 6 9 0 3 5 6 6 0 6 4 3 9 3 9 7 2 9 0 4 5 3 6 5 9 9 8 4 2 1 3 7 7\n",
            " 2 2 3 9 8 0 3 2 2 5 6 9 9 4 1 5 4 2 3 6 4 8 5 9 5 7 8 9 4 8 1 5 4 4 9 6 1 8 6 0 4 5 2 7 4 6 4 5 6 0 3 2 3 6 7 1 5 1 4 7 6 8 8 5 5 1 6 2 8 8 9 9 7 6 2 2 2 3 4 8 8 3 6 0 9 7 7 0 1\n",
            " 0 4 5 1 5 3 6 0 4 1 0 0 3 6 5 9 7 3 5 5 9 9 8 5 3 3 2 0 5 8 3 4 0 2 4 6 4 3 4 5 0 5 2 1 3 1 4 1 1 7 0 1 5 2 1 2 8 7 0 6 4 8 8 5 1 8 4 5 8 7 9 8 5 0 6 2 0 7 9 8 9 5 2 7 7 1 8 7 4\n",
            " 3 8 3 5 6 0 0 3 0 5 0 0 4 1 2 8 4 5 9 6 3 1 8 8 4 2 3 8 9 8 8 5 0 6 3 3 7 1 6 4 1 2 1 1 6 4 7 4 8 3 4 0 5 1 9 4 5 7 6 3 7 0 5 9 7 5 9 7 4 2 1 9 0 7 5 3 3 6 3 9 6 9 5 0 1 5 5 8 3\n",
            " 3 6 2 6 5]\n",
            "\n",
            "The mean absolute error on the test set is\t 2.557777777777778 \n",
            "\n",
            "------------\n",
            "Using the lrSchedual gradient variant\n",
            "-------------\n",
            "Epoch  0\n",
            "Initial starting points:  (65, 10)\n",
            "Gradient descent took 0 iterations to converge\n",
            "Initial starting points:  (65, 10)\n",
            "Gradient descent took 0 iterations to converge\n",
            "Epoch  1\n",
            "Initial starting points:  (65, 10)\n",
            "Gradient descent took 0 iterations to converge\n",
            "Initial starting points:  (65, 10)\n",
            "Gradient descent took 0 iterations to converge\n",
            "Finding the best parameters at the epoch of  1\n",
            "\n",
            "Fitting time is 0.17238306999206543\n",
            "\n",
            "Predicting the model...\n",
            "\n",
            "Prediction time is 0.0007479190826416016\n",
            "Accuracy: 0.07777777777777778\n",
            "Mean of predictions:  [2 1 1 6 1 2 6 1 6 1 0 6 4 5 7 1 1 1 2 6 0 1 6 0 2 2 1 0 2 1 0 6 1 0 9 2 2 0 0 2 2 1 0 1 2 2 1 5 2 6 2 0 6 6 2 2 2 2 2 0 6 0 6 2 0 6 1 2 1 2 6 2 1 4 1 2 1 2 2 2 6 2 2 1 6 6 2 9 0\n",
            " 2 2 1 5 1 2 1 2 2 7 5 0 1 6 1 7 1 6 6 0 1 1 1 4 0 1 1 0 6 6 1 1 6 6 0 2 1 1 1 6 5 0 6 1 6 6 2 2 7 0 2 2 1 0 2 6 2 6 2 2 2 2 2 2 2 2 7 1 2 1 2 1 2 2 6 0 1 1 2 2 6 6 2 0 1 1 1 1 1\n",
            " 1 1 1 2 6 6 1 1 9 6 2 2 2 7 0 2 9 1 2 2 7 1 6 6 2 7 1 2 7 5 0 6 6 7 2 2 0 1 6 2 9 6 2 6 0 2 0 6 2 6 1 1 1 2 1 7 2 2 0 1 2 5 2 2 2 1 2 2 2 5 6 1 1 2 2 1 1 1 2 2 1 1 2 2 2 1 0 6 1\n",
            " 5 2 2 2 6 1 2 2 7 1 6 5 2 2 2 2 1 1 1 2 2 2 5 6 1 1 0 2 6 5 1 7 2 2 7 2 0 1 7 2 2 6 1 1 2 5 0 1 2 6 2 2 6 2 0 2 6 1 2 6 0 2 7 1 1 2 0 6 2 6 2 1 1 6 2 2 2 6 6 2 2 6 2 7 0 7 6 0 0\n",
            " 1 7 2 2 2 2 2 1 2 6 2 2 5 1 2 1 1 6 6 2 1 2 1 2 0 1 1 7 2 1 1 6 6 2 1 1 6 1 5 9 2 1 0 0 2 0 0 0 2 1 0 2 6 0 2 0 2 1 2 1 7 2 1 2 6 1 2 1 0 2 2 6 6 0 6 2 1 2 1 2 2 6 6 2 1 2 1 1 1\n",
            " 1 2 1 2 6]\n",
            "\n",
            "Real values:  [6 9 3 7 2 1 5 2 5 2 1 9 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 9 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 5 5 4 7 0 4 5 5 9 9 0 2 3 8 0 6 4 4\n",
            " 9 1 2 8 3 5 2 9 0 4 4 4 3 5 3 1 3 5 9 4 2 7 7 4 4 1 9 2 7 8 7 2 6 9 4 0 7 2 7 5 8 7 5 7 7 0 6 6 4 2 8 0 9 4 6 9 9 6 9 0 3 5 6 6 0 6 4 3 9 3 9 7 2 9 0 4 5 3 6 5 9 9 8 4 2 1 3 7 7\n",
            " 2 2 3 9 8 0 3 2 2 5 6 9 9 4 1 5 4 2 3 6 4 8 5 9 5 7 8 9 4 8 1 5 4 4 9 6 1 8 6 0 4 5 2 7 4 6 4 5 6 0 3 2 3 6 7 1 5 1 4 7 6 8 8 5 5 1 6 2 8 8 9 9 7 6 2 2 2 3 4 8 8 3 6 0 9 7 7 0 1\n",
            " 0 4 5 1 5 3 6 0 4 1 0 0 3 6 5 9 7 3 5 5 9 9 8 5 3 3 2 0 5 8 3 4 0 2 4 6 4 3 4 5 0 5 2 1 3 1 4 1 1 7 0 1 5 2 1 2 8 7 0 6 4 8 8 5 1 8 4 5 8 7 9 8 5 0 6 2 0 7 9 8 9 5 2 7 7 1 8 7 4\n",
            " 3 8 3 5 6 0 0 3 0 5 0 0 4 1 2 8 4 5 9 6 3 1 8 8 4 2 3 8 9 8 8 5 0 6 3 3 7 1 6 4 1 2 1 1 6 4 7 4 8 3 4 0 5 1 9 4 5 7 6 3 7 0 5 9 7 5 9 7 4 2 1 9 0 7 5 3 3 6 3 9 6 9 5 0 1 5 5 8 3\n",
            " 3 6 2 6 5]\n",
            "\n",
            "The r^2 score is\t -0.8652392051551512\n",
            "\n",
            "The mean absolute error on the training set is\t 3.234595397178916\n",
            "\n",
            "The predicted y values for the test set are\t [2 1 1 6 1 2 6 1 6 1 0 6 4 5 7 1 1 1 2 6 0 1 6 0 2 2 1 0 2 1 0 6 1 0 9 2 2 0 0 2 2 1 0 1 2 2 1 5 2 6 2 0 6 6 2 2 2 2 2 0 6 0 6 2 0 6 1 2 1 2 6 2 1 4 1 2 1 2 2 2 6 2 2 1 6 6 2 9 0\n",
            " 2 2 1 5 1 2 1 2 2 7 5 0 1 6 1 7 1 6 6 0 1 1 1 4 0 1 1 0 6 6 1 1 6 6 0 2 1 1 1 6 5 0 6 1 6 6 2 2 7 0 2 2 1 0 2 6 2 6 2 2 2 2 2 2 2 2 7 1 2 1 2 1 2 2 6 0 1 1 2 2 6 6 2 0 1 1 1 1 1\n",
            " 1 1 1 2 6 6 1 1 9 6 2 2 2 7 0 2 9 1 2 2 7 1 6 6 2 7 1 2 7 5 0 6 6 7 2 2 0 1 6 2 9 6 2 6 0 2 0 6 2 6 1 1 1 2 1 7 2 2 0 1 2 5 2 2 2 1 2 2 2 5 6 1 1 2 2 1 1 1 2 2 1 1 2 2 2 1 0 6 1\n",
            " 5 2 2 2 6 1 2 2 7 1 6 5 2 2 2 2 1 1 1 2 2 2 5 6 1 1 0 2 6 5 1 7 2 2 7 2 0 1 7 2 2 6 1 1 2 5 0 1 2 6 2 2 6 2 0 2 6 1 2 6 0 2 7 1 1 2 0 6 2 6 2 1 1 6 2 2 2 6 6 2 2 6 2 7 0 7 6 0 0\n",
            " 1 7 2 2 2 2 2 1 2 6 2 2 5 1 2 1 1 6 6 2 1 2 1 2 0 1 1 7 2 1 1 6 6 2 1 1 6 1 5 9 2 1 0 0 2 0 0 0 2 1 0 2 6 0 2 0 2 1 2 1 7 2 1 2 6 1 2 1 0 2 2 6 6 0 6 2 1 2 1 2 2 6 6 2 1 2 1 1 1\n",
            " 1 2 1 2 6]\n",
            "The real y values for the test set are\t [6 9 3 7 2 1 5 2 5 2 1 9 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 9 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 5 5 4 7 0 4 5 5 9 9 0 2 3 8 0 6 4 4\n",
            " 9 1 2 8 3 5 2 9 0 4 4 4 3 5 3 1 3 5 9 4 2 7 7 4 4 1 9 2 7 8 7 2 6 9 4 0 7 2 7 5 8 7 5 7 7 0 6 6 4 2 8 0 9 4 6 9 9 6 9 0 3 5 6 6 0 6 4 3 9 3 9 7 2 9 0 4 5 3 6 5 9 9 8 4 2 1 3 7 7\n",
            " 2 2 3 9 8 0 3 2 2 5 6 9 9 4 1 5 4 2 3 6 4 8 5 9 5 7 8 9 4 8 1 5 4 4 9 6 1 8 6 0 4 5 2 7 4 6 4 5 6 0 3 2 3 6 7 1 5 1 4 7 6 8 8 5 5 1 6 2 8 8 9 9 7 6 2 2 2 3 4 8 8 3 6 0 9 7 7 0 1\n",
            " 0 4 5 1 5 3 6 0 4 1 0 0 3 6 5 9 7 3 5 5 9 9 8 5 3 3 2 0 5 8 3 4 0 2 4 6 4 3 4 5 0 5 2 1 3 1 4 1 1 7 0 1 5 2 1 2 8 7 0 6 4 8 8 5 1 8 4 5 8 7 9 8 5 0 6 2 0 7 9 8 9 5 2 7 7 1 8 7 4\n",
            " 3 8 3 5 6 0 0 3 0 5 0 0 4 1 2 8 4 5 9 6 3 1 8 8 4 2 3 8 9 8 8 5 0 6 3 3 7 1 6 4 1 2 1 1 6 4 7 4 8 3 4 0 5 1 9 4 5 7 6 3 7 0 5 9 7 5 9 7 4 2 1 9 0 7 5 3 3 6 3 9 6 9 5 0 1 5 5 8 3\n",
            " 3 6 2 6 5]\n",
            "\n",
            "The mean absolute error on the test set is\t 3.1644444444444444 \n",
            "\n"
          ]
        }
      ]
    }
  ]
}